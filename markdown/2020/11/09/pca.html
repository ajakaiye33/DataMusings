<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Practical Implementation of PCA | DataPonderings</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Practical Implementation of PCA" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Principal Component Analysis." />
<meta property="og:description" content="Principal Component Analysis." />
<link rel="canonical" href="https://ajakaiye33.github.io/DataMusings/markdown/2020/11/09/pca.html" />
<meta property="og:url" content="https://ajakaiye33.github.io/DataMusings/markdown/2020/11/09/pca.html" />
<meta property="og:site_name" content="DataPonderings" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-11-09T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"Principal Component Analysis.","url":"https://ajakaiye33.github.io/DataMusings/markdown/2020/11/09/pca.html","@type":"BlogPosting","headline":"Practical Implementation of PCA","dateModified":"2020-11-09T00:00:00-06:00","datePublished":"2020-11-09T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://ajakaiye33.github.io/DataMusings/markdown/2020/11/09/pca.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/DataMusings/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://ajakaiye33.github.io/DataMusings/feed.xml" title="DataPonderings" /><link rel="shortcut icon" type="image/x-icon" href="/DataMusings/images/favicon.ico"><link href="https://cdn.jsdelivr.net/npm/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/DataMusings/">DataPonderings</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/DataMusings/about/">About Me</a><a class="page-link" href="/DataMusings/search/">Search</a><a class="page-link" href="/DataMusings/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Practical Implementation of PCA</h1><p class="page-description">Principal Component Analysis.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-11-09T00:00:00-06:00" itemprop="datePublished">
        Nov 9, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      4 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/DataMusings/categories/#markdown">markdown</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h3"><a href="#introduction">Introduction</a></li>
<li class="toc-entry toc-h3"><a href="#initialization--standardization--transformation">Initialization &amp; Standardization &amp; Transformation</a></li>
<li class="toc-entry toc-h3"><a href="#explained-component-summary-report">Explained Component Summary Report</a></li>
<li class="toc-entry toc-h3"><a href="#decision-how-many-components">Decision: How many components?</a></li>
<li class="toc-entry toc-h3"><a href="#conclusion">Conclusion</a></li>
</ul><h3 id="introduction">
<a class="anchor" href="#introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction</h3>

<p>The first time I encountered PCA(Principal Component Analysis) as a concept was in a <em>Thinkful’s</em> <em>Introduction to Data Science</em> Course. As much as
I can remember I was baffled and bewildered. For weeks I couldn’t make
head or tail of what it means and the ‘job’ it is meant to do in machine learning and data science. However, a considerable milestone was attained when my mentor concurred with my suggestion that it is similar to the Pareto Principle or the law of the vital few. The law of the vital few states that ‘for many outcomes, roughly 80% of consequences come from 20% of the causes’. In other words, <code class="language-plaintext highlighter-rouge">80%</code> of the components of a subject lies in <code class="language-plaintext highlighter-rouge">20%</code> of such components.</p>

<p>When ‘PP’ is juxtaposed with Principal Component Analysis, the problem is then reframed as “how to reduce the component that makes up a subject to as few components as possible without losing the meaning of such a subject”. Thus Principal Component Analysis from the perspective of machine learning and Data Science is a complexity-reduction technique that tries to reduce a set of variables down to a smaller set of components that represent most of the information in the variables. Through this transformation, a large chunk of the information across the full dataset is effectively compressed in fewer feature columns. It is the most popular <em>Dimensionality Reduction Algorithm</em> used in various operations like:</p>

<ul>
  <li>Noise Filtering</li>
  <li>Feature extraction</li>
  <li>visualization</li>
  <li>Gene Data analysis</li>
  <li>Stock market prediction e.t.c</li>
</ul>

<p>Keeping with the spirit of this blog, I shall not bore you with unnecessary mathematical theories/jargon about PCA; we shall concentrate on how I often apply it in model building exercise. However, you can read more about the mathematical foundation of PCA. There is <a href="'https://projects.ics.forth.gr/mobile/pca.pdf'">A Tutorial on Principal Component Analysis</a> by Jonathan Shlens at the Salk Institute.</p>

<p>To this end, in this post, I shall cover PCA in the following sub-categories/steps:</p>
<ul>
  <li>Initialization, Standardization &amp; Transformation</li>
  <li>Explained Component Summary Report</li>
  <li>Decision: How many components?</li>
</ul>

<p>For effective coverage we shall demonstrate with  <a href="'https://raw.githubusercontent.com/Thinkful-Ed/data-201-resources/master/ESS_practice_data/ESSdata_Thinkful.csv'">simple dataset</a></p>

<h3 id="initialization--standardization--transformation">
<a class="anchor" href="#initialization--standardization--transformation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Initialization &amp; Standardization &amp; Transformation</h3>
<pre><code class="language-Python"> #import necessary package and Modules
 import pandas as pd
 import numpy as np
 import seaborn as sns
 import matplotlib.pyplot as plt
 from sklearn.preprocessing import StandardScaler
 from sklearn.decomposition import PCA
</code></pre>
<p>To perform a PCA operation effectively the dataset must be Standardized and an object must be created from the PCA class</p>

<pre><code class="language-Python">#read data unto pandas
mydata = pd.read_csv('https://raw.githubusercontent.com/Thinkful-Ed/data-201-resources/master/ESS_practice_data/ESSdata_Thinkful.csv')
mydata.head()
</code></pre>

<p><img src="/images/datahead.png" alt=""></p>

<p>For brevity we shall select four features only which is more than enough to demonstrate.</p>

<pre><code class="language-Python">mydata.columns
</code></pre>
<p><img src="/images/columns.png" alt=""></p>

<pre><code class="language-Python">mydata[['tvtot', 'ppltrst','pplfair','pplhlp']]
mydata.head()
</code></pre>
<p><img src="/images/newcols.png" alt=""></p>

<pre><code class="language-Python">#Standardized data and place result into a pandas DataFrame
mydata_std = pd.DataFrame(StandardScaler().fit_transform(mydata), columns= mydata.columns)

#initte PCA
pca = PCA(n_components=4)

#get pcs version of data
mydata_pca = pd.DataFrame(pca.fit_transform(mydata_std), columns=mydata.columns)
mydata_std.head()
mydata_pca.head()
</code></pre>
<p><img src="/images/std_data.png" alt=""></p>

<p><img src="/images/pca_data.png" alt=""></p>

<h3 id="explained-component-summary-report">
<a class="anchor" href="#explained-component-summary-report" aria-hidden="true"><span class="octicon octicon-link"></span></a>Explained Component Summary Report</h3>

<p>As a convention, normally I produce a report which makes the PCA operations easy to digest. And we do this with the three methods:</p>

<ul>
  <li>explained variance: These are the variance components accounted for by each principal component, in descending order. The sum of these variance components is the total variance.</li>
  <li>explained variance ratio: These are the proportion of total data variance accounted for by each principal component, in descending order. They sum to <code class="language-plaintext highlighter-rouge">1.0</code>
</li>
  <li>Singular values: These are used to calculate the variance components.</li>
</ul>

<pre><code class="language-Python">pca.explained_variance_
pca.explained_variance_ratio_
pca.singular_values_
</code></pre>

<p><img src="/images/pca_methods.png" alt=""></p>

<pre><code class="language-Python"># summary Report
data = { 'Singular Value':pca.singular_values_,
         'Variance':pca.explained_variance_,
         '%Variance':pca.explained_variance_ratio_*100 }
var_report = pd.DataFrame( data )
var_report[ 'Cum Sum' ] = var_report[ '%Variance' ].cumsum()
print( 'Number of observations: n = {}'.format( pca.n_samples_ ) )
var_report
</code></pre>
<p><img src="/images/report_pca.png" alt=""></p>

<p>The <em>Variance</em> is the contribution to the total variance of the data and is based on the singular values. Thus the sum of the <em>Variance</em> contribution is the total variance. The first principal component accounts for <code class="language-plaintext highlighter-rouge">51.5%</code> of the variance and the first two account for <code class="language-plaintext highlighter-rouge">75.8%</code>.</p>

<h3 id="decision-how-many-components">
<a class="anchor" href="#decision-how-many-components" aria-hidden="true"><span class="octicon octicon-link"></span></a>Decision: How many components?</h3>

<p>The biggest decision to make when running a PCA is how many components to keep.
There are several rules to guide us in choosing the number of the component to keep. The most straightforward is to keep components with <em>eigenvalues</em> or where <em>Singular value</em> is greater than 1 as these components add value. (because they contain more information than a single variable)</p>

<p>Another rule is <em>variance cutoffs</em> where we only keep components that explained at least <code class="language-plaintext highlighter-rouge">x%</code> of the variance in the data.</p>

<p>Yet another rule is to visualize the eigenvalues in order from highest to lowest connecting them with a line. This is also called the scree plot.</p>

<pre><code class="language-Python">#plot scree plot
plt.plot(var_report['pctVariance'])
</code></pre>
<p><img src="/images/screeplot.png" alt=""></p>

<p>From visual inspection, we shall keep all the component whose eigenvalues or pctVariance values falls above the point where the slope of the line changes the most drastically.This is also called the ‘elbow’ point.Thus <code class="language-plaintext highlighter-rouge">24%</code> and above will form our components.</p>
<h3 id="conclusion">
<a class="anchor" href="#conclusion" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusion</h3>
<p>In the course of this practical post, we have come to understand the significance of PCA operation in the machine learning process. We also demonstrate the simplest and effective way of analyzing the PCA decision process, which, actually is the most important step, as the primary purpose for undertaking a PCA operation invariably is to reduce features!</p>

<p>I hope you have learned something today.
Bye!</p>

  </div><a class="u-url" href="/DataMusings/markdown/2020/11/09/pca.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/DataMusings/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/DataMusings/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/DataMusings/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Data gymnastics... Enter Zen from there!.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://twitter.com/ajakzheddy" title="ajakzheddy"><svg class="svg-icon grey"><use xlink:href="/DataMusings/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
