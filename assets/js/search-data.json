{
  
    
        "post0": {
            "title": "Data Restructure",
            "content": ". Introduction . Some couple of weeks ago, I can remember I drew your attention to the bitter truth about Data Science. In that piece, I posited that beyond the &#39;sexiness&#39; and other euphemisms wrapped around the practice of Data Science, Data Science is Data wrangling and cleaning! How else can one describe a profession where over 90 per cent of its activities revolves around data cleaning and wrangling? . I&#39;m happy my mentors drilled this concept of Data Science into my subconscious mind at the very beginning of my encounter with Data Science. The view that Data Cleaning and wrangling form a more significant part of Data Science practice has not only humbled me but has helped me to develop a growth mindset in my quest to learn and practice Data Science. To this end, the majority of the data in the wild will need a significant amount of restructuring before commencing a more detailed analysis. . My attention this week is turned to the concept and practice of data restructuring or &#39;tidy data&#39;. Tidy data is one topic; in my opinion, critical to building a solid foundation in Data Science. Data restructure as a procedure has grown in lips and bounds so much so that in data science and analysis parlance, thanks to Hadley Wickham is now popularly referred to as Tidy Data . What is Tidy Data? . The concept of Tidy Data will be best understood when demonstrated than theoretically explained. To this end, in the course of this piece, I will use examples from real-life data to make the topic more interactive and possibly hands-on. . According to the father of Tidy Data, a dataset is messy if it does not meet the following guidelines: . Each variable from a column | Each observation from a row | Each type of observational unit forms a table | . Messy or Untidy Data . Since the taste of the pudding is in the eating, we will display an example of messy data in the real-world. Our dataset, for this post, is the record of students in private and public schools performance in the West African Examination Council (WAEC) Examination from the year 2016 to 2018. This is how the data set look like when viewed with MS Excel: . Types of Messy Data . The first step to resolving messy data is to recognize it when it exists. However, Hadley explicitly mentioned five of the most common types of dirty data: . Columns names are values, not variables names | Multiple variables are stored in column names | Variables are stored in both rows and columns | Multiple types of observational units are stored in the same table | A single observational unit is stored in multiple tables | . Tidy Data Procedure . Tidying data does not typically involve changing the value of a dataset, filling in missing values or doing any analysis. Tidying data consist of changing the shape or structure of the data to meet the tidy data principle . Read Data into Pandas . As customary, Pandas is the go-to tool for cleaning, restructuring, wrangling and visualizing data. Our goal, as far as this dataset is concerned, is to get for each year the private and public school performance of both sex and their respective states. Without doubt, to get our dataset from its present structure to the structure indicated above, we will perform some restructuring and tidying. Thus from the current dataset, we would extract the following variable/columns and variable values. . States | Year | Sex | Five_credit_and_above - Percentage of student that earned five credit and above | . import sys;sys.path.extend([r&quot;/Users/user/anaconda3/envs/wrangling_data/lib/python3.7/site-packages&quot;]) import pandas as pd import numpy as np import matplotlib.pyplot as plt messy_data_private_2016 = pd.read_excel(&#39;./The-Data-Wrangling-Workshop/Chapter09/datasets/waec_2016_2018.xlsx&#39;, sheet_name=&#39;private 2016&#39;) messy_data_public_2016 = pd.read_excel(&#39;./The-Data-Wrangling-Workshop/Chapter09/datasets/waec_2016_2018.xlsx&#39;, sheet_name=&#39;Public 2016&#39;) messy_data_private_2017 = pd.read_excel(&#39;./The-Data-Wrangling-Workshop/Chapter09/datasets/waec_2016_2018.xlsx&#39;, sheet_name=&#39;Private 2017&#39;) messy_data_public_2017 = pd.read_excel(&#39;./The-Data-Wrangling-Workshop/Chapter09/datasets/waec_2016_2018.xlsx&#39;, sheet_name=&#39;public 2017&#39;) messy_data_private_2018 = pd.read_excel(&#39;./The-Data-Wrangling-Workshop/Chapter09/datasets/waec_2016_2018.xlsx&#39;, sheet_name=&#39;Private 2018&#39;) messy_data_public_2018 = pd.read_excel(&#39;./The-Data-Wrangling-Workshop/Chapter09/datasets/waec_2016_2018.xlsx&#39;, sheet_name=&#39;Public 2018&#39;) . messy_data_private_2016.head() . WEST AFRICAN EXAMINATIONS COUNCIL Unnamed: 1 Unnamed: 2 Unnamed: 3 Unnamed: 4 Unnamed: 5 Unnamed: 6 Unnamed: 7 Unnamed: 8 Unnamed: 9 Unnamed: 10 Unnamed: 11 Unnamed: 12 Unnamed: 13 Unnamed: 14 Unnamed: 15 . 0 RESULT STATISTICS FOR 5 CREDITS IN ENGLISH, MA... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 WASSCE PRIVATE CANDIDATES EXAMINATION 2016 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 STATE | TOTAL NUMBER SAT | NaN | NaN | 5 CREDITS AND ABOVE INCLUDING ENGLISH LANGUAGE | NaN | NaN | 5 CREDITS AND ABOVE INCLUDING MATHEMATICS | NaN | NaN | 5 CREDITS &amp; ABOVE INCLUDING MATHEMATICS &amp; ENGL... | NaN | NaN | % OF 5 CREDITS AND ABOVE | NaN | NaN | . 3 NaN | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | . 4 ABIA | 309 | 276 | 585 | 65 | 59 | 124 | 39 | 27 | 66 | 35 | 25 | 60 | 11.3269 | 9.05797 | 10.2564 | . messy_data_public_2016.head() . WEST AFRICAN EXAMINATIONS COUNCIL Unnamed: 1 Unnamed: 2 Unnamed: 3 Unnamed: 4 Unnamed: 5 Unnamed: 6 Unnamed: 7 Unnamed: 8 Unnamed: 9 Unnamed: 10 Unnamed: 11 Unnamed: 12 Unnamed: 13 Unnamed: 14 Unnamed: 15 . 0 RESULT STATISTICS FOR 5 CREDITS IN ENGLISH, MA... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 WASSCE CANDIDATES EXAMINATION 2016 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 STATE | TOTAL NUMBER SAT | NaN | NaN | 5 CREDITS AND ABOVE INCLUDING ENGLISH LANGUAGE | NaN | NaN | 5 CREDITS AND ABOVE INCLUDING MATHEMATICS | NaN | NaN | 5 CREDITS &amp; ABOVE INCLUDING MATHEMATICS &amp; ENGL... | NaN | NaN | % OF 5 CREDITS &amp; ABOVE INCLUDING MATHEMATICS &amp;... | NaN | NaN | . 3 NaN | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | . 4 ABIA | 24322 | 27405 | 51727 | 20403 | 23564 | 43967 | 18485 | 20688 | 39173 | 17264 | 19526 | 36790 | 70.981 | 71.2498 | 71.1234 | . messy_data_private_2017.head() . WEST AFRICAN EXAMINATIONS COUNCIL Unnamed: 1 Unnamed: 2 Unnamed: 3 Unnamed: 4 Unnamed: 5 Unnamed: 6 Unnamed: 7 Unnamed: 8 Unnamed: 9 Unnamed: 10 Unnamed: 11 Unnamed: 12 Unnamed: 13 Unnamed: 14 Unnamed: 15 . 0 RESULT STATISTICS FOR 5 CREDITS IN ENGLISH ,M... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 WASSCE PRIVATE CANDIDATES EXAMINATION 2017 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 STATE | TOTAL NUMBER SAT | NaN | NaN | 5 CREDITS AND ABOVE INCLUDING ENGLISH LANGUAGE | NaN | NaN | 5 CREDITS AND ABOVE INCLUDING MATHEMATICS | NaN | NaN | 5 CREDITS &amp; ABOVE INCLUDING MATHEMATICS &amp; ENGL... | NaN | NaN | % OF 5 CREDITS &amp; ABOVE INCLUDING MATHEMATICS &amp;... | NaN | NaN | . 3 NaN | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | . 4 ABIA | 331 | 279 | 610 | 128 | 116 | 244 | 106 | 105 | 211 | 96 | 99 | 195 | 29.003 | 35.4839 | 31.9672 | . messy_data_public_2017.head() . WEST AFRICAN EXAMINATIONS COUNCIL Unnamed: 1 Unnamed: 2 Unnamed: 3 Unnamed: 4 Unnamed: 5 Unnamed: 6 Unnamed: 7 Unnamed: 8 Unnamed: 9 Unnamed: 10 Unnamed: 11 Unnamed: 12 Unnamed: 13 Unnamed: 14 Unnamed: 15 . 0 RESULT STATISTICS FOR 5 CREDITS IN ENGLISH ,MA... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 WASSCE CANDIDATES EXAMINATION 2017 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 STATE | TOTAL NUMBER SAT | NaN | NaN | 5 CREDITS AND ABOVE INCLUDING ENGLISH LANGUAGE | NaN | NaN | 5 CREDITS AND ABOVE INCLUDING MATHEMATICS | NaN | NaN | 5 CREDITS &amp; ABOVE INCLUDING MATHEMATICS &amp; ENGL... | NaN | NaN | % OF 5 CREDITS &amp; ABOVE INCLUDING MATHEMATICS &amp;... | NaN | NaN | . 3 NaN | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | . 4 ABIA | 23649 | 26023 | 49672 | 19317 | 21712 | 41029 | 20727 | 22931 | 43658 | 17928 | 20084 | 38012 | 75.8087 | 77.1779 | 76.526 | . messy_data_private_2018.head() . WEST AFRICAN EXAMINATIONS COUNCIL Unnamed: 1 Unnamed: 2 Unnamed: 3 Unnamed: 4 Unnamed: 5 Unnamed: 6 Unnamed: 7 Unnamed: 8 Unnamed: 9 ... Unnamed: 16374 Unnamed: 16375 Unnamed: 16376 Unnamed: 16377 Unnamed: 16378 Unnamed: 16379 Unnamed: 16380 Unnamed: 16381 Unnamed: 16382 Unnamed: 16383 . 0 RESULT STATISTICS FOR 5 CREDITS IN ENGLISH ,M... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 WASSCE PRIVATE CANDIDATES EXAMINATION 2018 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 STATE | TOTAL NUMBER SAT | NaN | NaN | 5 CREDITS AND ABOVE INCLUDING ENGLISH LANGUAGE | NaN | NaN | 5 CREDITS AND ABOVE INCLUDING MATHEMATICS | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 3 NaN | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 4 ABIA | 308 | 291 | 599 | 176 | 176 | 352 | 172 | 170 | 342 | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 5 rows × 16384 columns . messy_data_public_2018.head() . WEST AFRICAN EXAMINATIONS COUNCIL Unnamed: 1 Unnamed: 2 Unnamed: 3 Unnamed: 4 Unnamed: 5 Unnamed: 6 Unnamed: 7 Unnamed: 8 Unnamed: 9 Unnamed: 10 Unnamed: 11 Unnamed: 12 Unnamed: 13 Unnamed: 14 Unnamed: 15 . 0 RESULT STATISTICS FOR 5 CREDITS IN ENGLISH ,M... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 WASSCE CANDIDATES EXAMINATION 2018 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 STATE | TOTAL NUMBER SAT | NaN | NaN | 5 CREDITS AND ABOVE INCLUDING ENGLISH LANGUAGE | NaN | NaN | 5 CREDITS AND ABOVE INCLUDING MATHEMATICS | NaN | NaN | 5 CREDITS &amp; ABOVE INCLUDING MATHEMATICS &amp; ENGL... | NaN | NaN | % of 5 CREDITS AND ABOVE INCLUDING MATHEMATICS... | NaN | NaN | . 3 NaN | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | . 4 ABIA | 22502 | 24966 | 47468 | 19711 | 22056 | 41767 | 20239 | 22426 | 42665 | 18484 | 20572 | 39056 | 82.1438 | 82.4001 | 82.2786 | . Remove/skip Useless Values and Variables . From the above MS Excel display of the data we can see that we need to skip some rows of values that are not useful for our desired data structure . skip_rows_private_2016 = pd.read_excel(&#39;./The-Data-Wrangling-Workshop/Chapter09/datasets/waec_2016_2018.xlsx&#39;, sheet_name=&#39;private 2016&#39;,header=None,skiprows=[0,1,2]) rename_col = skip_rows_private_2016.iloc[2:39,[0, 13,14]] rename_col = rename_col.rename(columns={0:&#39;States&#39;, 13:&#39;Male&#39;,14:&#39;Female&#39;}) rename_col[&#39;School_type&#39;] = &#39;private&#39; rename_col[&#39;Year&#39;] = 2016 #public 2016 skip_rows_public_2016 = pd.read_excel(&#39;./The-Data-Wrangling-Workshop/Chapter09/datasets/waec_2016_2018.xlsx&#39;, sheet_name=&#39;Public 2016&#39;,header=None, skiprows=[0,1,2]) rename_col_2016_b = skip_rows_public_2016.iloc[2:39,[0,13,14]] rename_col_2016_b = rename_col_2016_b.rename(columns={0:&#39;States&#39;, 13:&#39;Male&#39;, 14:&#39;Female&#39;}) rename_col_2016_b[&#39;School_type&#39;] = &#39;public&#39; rename_col_2016_b[&#39;Year&#39;] = 2016 #private 2017 skip_rows_private_2017 = pd.read_excel(&#39;./The-Data-Wrangling-Workshop/Chapter09/datasets/waec_2016_2018.xlsx&#39;, sheet_name=&#39;Private 2017&#39;,header=None,skiprows=[0,1,2]) rename_col_2017 = skip_rows_private_2017.iloc[2:39,[0, 13,14]] rename_col_2017 = rename_col_2017.rename(columns={0:&#39;States&#39;, 13:&#39;Male&#39;,14:&#39;Female&#39;}) rename_col_2017[&#39;School_type&#39;] = &#39;private&#39; rename_col_2017[&#39;Year&#39;] = 2017 #public 2017 skip_rows_public_2017 = pd.read_excel(&#39;./The-Data-Wrangling-Workshop/Chapter09/datasets/waec_2016_2018.xlsx&#39;, sheet_name=&#39;public 2017&#39;,header=None, skiprows=[0,1,2]) rename_col_2017_public = skip_rows_public_2017.iloc[2:39,[0,13,14]] rename_col_2017_public = rename_col_2017_public.rename(columns={0:&#39;States&#39;, 13:&#39;Male&#39;, 14:&#39;Female&#39;}) rename_col_2017_public[&#39;School_type&#39;] = &#39;public&#39; rename_col_2017_public[&#39;Year&#39;] = 2017 #private 2018 skip_rows_private_2018 = pd.read_excel(&#39;./The-Data-Wrangling-Workshop/Chapter09/datasets/waec_2016_2018.xlsx&#39;, sheet_name=&#39;Private 2018&#39;,header=None,skiprows=[0,1,2]) rename_col_2018_private = skip_rows_private_2018.iloc[2:39,[0, 13,14]] rename_col_2018_private = rename_col_2018_private.rename(columns={0:&#39;States&#39;, 13:&#39;Male&#39;,14:&#39;Female&#39;}) rename_col_2018_private[&#39;School_type&#39;] = &#39;private&#39; rename_col_2018_private[&#39;Year&#39;] = 2018 #public 2018 skip_rows_public_2018 = pd.read_excel(&#39;./The-Data-Wrangling-Workshop/Chapter09/datasets/waec_2016_2018.xlsx&#39;, sheet_name=&#39;Public 2018&#39;,header=None, skiprows=[0,1,2]) rename_col_2018_public = skip_rows_public_2018.iloc[2:39,[0,13,14]] rename_col_2018_public = rename_col_2018_public.rename(columns={0:&#39;States&#39;, 13:&#39;Male&#39;, 14:&#39;Female&#39;}) rename_col_2018_public[&#39;School_type&#39;] = &#39;public&#39; rename_col_2018_public[&#39;Year&#39;] = 2018 . Transform to Tidy Data . From the above current version of our dataset, it is evident that we have made significant progress. We have a data-frame of the percentages of males and females with their respective states that earned five credit and above. Going by the tidy data guideline, our data-frame need to undergo some structure adjustment and re-alignment. Since our goal is to merge all the data in all the sheet into one table, i.e. data-frame, other sheets would pass through similar procedures. . tidy_private_2016 = rename_col.melt(id_vars=[&#39;States&#39;], value_vars=[&#39;Male&#39;, &#39;Female&#39;],var_name=&#39;Sex&#39;,value_name=&#39;pct_five_credit_above&#39;) . tidy_private_2016 . States Sex pct_five_credit_above . 0 ABIA | Male | 11.3269 | . 1 ABUJA | Male | 6.53595 | . 2 ADAMAWA | Male | 6.42857 | . 3 AKWA IBOM | Male | 12.0308 | . 4 ANAMBRA | Male | 16.7679 | . ... ... | ... | ... | . 69 RIVERS | Female | 47.2096 | . 70 SOKOTO | Female | 12.9032 | . 71 TARABA | Female | 0 | . 72 YOBE | Female | 0 | . 73 ZAMFARA | Female | 0 | . 74 rows × 3 columns . Conclusion . From the above version of our dataset, we can see that the current version satisfies all the tidy data guidelines and principles. Thus our dataset is transformed from wide to long. We have in the course of this post demonstrated data cleaning, wrangling and restructuring. However, more activities can be still possible to clean and restructure to fine-tune the dataset further. Nevertheless, we would conclude this post here as we have achieved the objective of demonstrating how data tidying using pandas. I hope you have learned something today. .",
            "url": "https://ajakaiye33.github.io/DataMusings/fastpages/jupyter/2020/12/07/tidy.html",
            "relUrl": "/fastpages/jupyter/2020/12/07/tidy.html",
            "date": " • Dec 7, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Feature Selection In The Machine Learning Process",
            "content": "Introduction . Recently I had the rare privilege to encounter a brilliant and concise perspective of machine learning from an equally experienced practitioner in the field. According to my mentor, machine learning is the &quot;semi-automatic process of extracting information from Data&quot;. I have seen similar definitions and perspectives of the concept of machine learning, as there is no universally accepted definition yet. However, this particular definition caught my ears and attention because of the use of semi-automatic phrase! Understandably I was equally curious about the meaning of semi-automatic in the context of the machine learning process. . As a way of summary, machine learning is semi-automatic because the process involves two parts: qualitative and quantitative. The qualitative activities are those activities we often refer to as pre-modelling activities. More or less those activities that at best falls under the realm of &#39;art&#39; of data science, subjectively carried out to pre-process and shape the data in a manner amenable to a model, the quantitative aspect of the machine learning process. Some of these qualitative processes, like &#39;Feature Engineering&#39;, creates another problem of high dimensionality, which invariably leads to increased complexity,over-fitting and consequently poor performance of the model. . There are a plethora of ways devised to handle the problem of the high dimensionality of data. Feature selection is one of such useful methods. Feature selection, simply put, is a process of pruning the features or variables of a dataset to the &#39;necessary few&#39; that have high explained ratios. You can do Feature selection in three ways: . Univariate Statistics Process | Iterative Process (Recursive Feature Elimination-RFE) | Model-Based Feature Selection Process | . Given our sub-topic, we shall dwell on the Model-driven Process of Feature Selection in this post. . The Model Process of Feature Selection . Making feature selection with the aid of algorithms is one of the most straightforward and direct processes of feature selection. I must stress here that while it is one of the convenient ways of feature selection, not all algorithms are suited for it. In essence, Model-driven feature selection leaves the decision process of feature selection and de-selection to the whims and caprices of tree-base algorithms. . Model-driven feature selection is done in the following simple steps. . Have your Dataset at hand | split your dataset into training and test sets | Initialized your model | Fit the model with the training set of your dataset | Extract the important features | Adjust dataset according and proceed to the model building stage | . Model -Driven Feature Selection - Demonstration . We need dataset and algorithm to demonstrate the model-based feature selection effectively. To this end, we shall look to the versatile and comprehensive sci-kit learn python package. . Import the &#39;Select from model&#39; class and other necessary modules . import sys;sys.path.extend([r&quot;/Users/user/anaconda3/envs/wrangling_data/lib/python3.7/site-packages&quot;]) . from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split from sklearn.feature_selection import SelectFromModel from sklearn.ensemble import RandomForestClassifier import numpy as np import matplotlib.pyplot as plt cancer = load_breast_cancer() . cancer.data.shape . (569, 30) . make_up_num = np.random.RandomState(42) noise = make_up_num.normal(size=(len(cancer.data),50)) #first thirty features are from the data whereas the next 50 features are noise stak_noise = np.hstack([cancer.data, noise]) . X_train,X_test, y_train, y_test = train_test_split(stak_noise, cancer.target, random_state=42, test_size=.50) . cancer.feature_names . array([&#39;mean radius&#39;, &#39;mean texture&#39;, &#39;mean perimeter&#39;, &#39;mean area&#39;, &#39;mean smoothness&#39;, &#39;mean compactness&#39;, &#39;mean concavity&#39;, &#39;mean concave points&#39;, &#39;mean symmetry&#39;, &#39;mean fractal dimension&#39;, &#39;radius error&#39;, &#39;texture error&#39;, &#39;perimeter error&#39;, &#39;area error&#39;, &#39;smoothness error&#39;, &#39;compactness error&#39;, &#39;concavity error&#39;, &#39;concave points error&#39;, &#39;symmetry error&#39;, &#39;fractal dimension error&#39;, &#39;worst radius&#39;, &#39;worst texture&#39;, &#39;worst perimeter&#39;, &#39;worst area&#39;, &#39;worst smoothness&#39;, &#39;worst compactness&#39;, &#39;worst concavity&#39;, &#39;worst concave points&#39;, &#39;worst symmetry&#39;, &#39;worst fractal dimension&#39;], dtype=&#39;&lt;U23&#39;) . select = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42)) #The select for model class select all features that have an important measureof tge features #(as provided by the supervised model greater thanthe provided threshold) . select.fit(X_train, y_train) trans_xtrain = select.transform(X_train) print(f&#39;X_train.shape:{X_train.shape}&#39;) print(f&#39;trans_xtrain.shape:{trans_xtrain.shape}&#39;) . X_train.shape:(284, 80) trans_xtrain.shape:(284, 20) . Model Discard useless and irrelevant Features . From the above operation, our original data had 30 features. We generated additional 50 fake features to test the efficacy of the process. From our above result, upon transforming the training set, the number of features drastically reduced to 20 attributes. From the preceding process, the model didn&#39;t only do away with the 50 fictitious features it equally discarded 10 features from our original 30 features! . mask = select.get_support() #Visualize the mask: black is True and white is False plt.matshow(mask.reshape(1,-1), cmap=&#39;gray_r&#39;) plt.xlabel(&#39;Sample index&#39;) plt.yticks(()) . ([], []) . Plot feature important from trained model . Another way is to use the important feature method to visualize the importance of features. From the bellow bar graph we can see that the most important feature is the worst concave points whereas the least importance feature is the Symmetry error? . cancer = load_breast_cancer() X_train,X_test,y_train,y_test = train_test_split(cancer.data, cancer.target,random_state=42) forest = RandomForestClassifier(n_estimators=100, random_state=42) forest.fit(X_train,y_train) . RandomForestClassifier(random_state=42) . forest.feature_importances_ . array([0.03971058, 0.01460399, 0.05314639, 0.04277978, 0.00816485, 0.01140166, 0.08321459, 0.0902992 , 0.00443533, 0.00443395, 0.01951684, 0.00459978, 0.00868228, 0.04355077, 0.00464415, 0.0036549 , 0.00701442, 0.00504716, 0.00371411, 0.00658253, 0.08127686, 0.01649014, 0.07138828, 0.12319232, 0.01033481, 0.01580059, 0.03174022, 0.17229521, 0.01310266, 0.00518165]) . def plot_feature_importance_cancer(model): n_features = cancer.data.shape[1] fig, ax = plt.subplots(figsize=(12,8)) plt.barh(range(n_features),model.feature_importances_, align=&#39;center&#39;) plt.yticks(np.arange(n_features),cancer.feature_names) plt.xlabel(&#39;Feature Importance&#39;) plt.ylabel(&#39;Feature&#39;) . plot_feature_importance_cancer(forest) . Conclusion . In the course of this post, we emphasized the relevance of feature importance in the machine learning process. Feature selection is such a necessary process that I make bold to say any good model worth its salt must have undergone feature selection in one way or another. We can also appreciate the ease of using a model to effect feature selection. I hope you have learned something today. Bye! .",
            "url": "https://ajakaiye33.github.io/DataMusings/jupyter/2020/11/30/Feature-Selection.html",
            "relUrl": "/jupyter/2020/11/30/Feature-Selection.html",
            "date": " • Nov 30, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Data Wrangling Fundamentals",
            "content": "Within the data science ecosystem, there is an overwhelming consensus that over ninety-five per cent of the data science process is data cleaning and wrangling. For such an important aspect of data science, you would expect greater attention, but the reality is that like human nature, we are almost always attracted to the shining parts of things. Consequently, beginners from the very beginning of their sojourn towards the unknown are lead astray. Do not be deceived; data science is data cleaning and data wrangling! As a veritable means of closing the yarning knowledge gap in the data wrangling and cleaning craft, I have decided to extract data-wrangling exercises from different resources. Thus this notebook and subsequent ones would highlight these exercises as well as their real-life application in the field of data science. . import numpy as np import pandas as pd . a = np.random.randn(5,3) a . array([[-1.12506181, -0.32504384, 1.67238151], [-0.04049187, -1.85380366, -1.47251248], [-1.39307679, -2.52397974, 0.96097851], [ 0.74104312, -0.14694375, -0.25140089], [-1.2898671 , -0.72692951, -1.49530227]]) . Steps of Data Wrangling . Scraping raw data from multiple sources(including web and database table) | Imputing(replacing missing data using various techniques), formating, and transforming-basically making it ready to be used in the modeling process | Handling read/write errors | Detecting outliers | Performing quick visualizations(plotting) and basic statistical analyses to judge the quality of formatted data | . Accessing The List Members Exercise:1.01 . ssn = list(pd.read_csv(&#39;./The-Data-Wrangling-Workshop/chapter01/datasets/ssn.csv&#39;)) . ssn[0] . ssn[3] . ssn[len(ssn)- 1] . ssn[-1] . ssn[:3] . ssn[-2:] . ssn[:-2] . ssn[-1::-1] . ssn[:] . Generating and Iterating through a List: 1.02 . ssn_2 = [] for i in ssn: ssn_2.append(i) ssn_2 . ssn3 = [&#39;soc: &#39;+ x for x in ssn_2] ssn3 . i = 0 while i &lt; len(ssn3): print(ssn3[i]) i += 1 . numbers = [x for x in ssn3 if &#39;5&#39; in x] numbers . ssn_4 = [&#39;102-90-0314&#39;,&#39;247-17-2338&#39;,&#39;318-22-2760&#39;] ssn_5 = ssn_4 + ssn ssn_5 . ssn_2.extend(ssn_4) ssn_2 . # for x in ssn_2: # for y in ssn_5: # print(str(x) + &#39;,&#39; + str(y)) . Iterating Over a List and Checking Membership: 1.03 . car_model = list(pd.read_csv(&#39;./The-Data-Wrangling-Workshop/chapter01/datasets/car_models.csv&#39;)) car_model . list_1 = [x for x in car_model] for i in range(0,len(list_1)): print(list_1[i]) . for i in list_1: print(i) . &#39;D150&#39; in list_1, &#39;Mustang&#39; in list_1 . Sorting A List: Exercise 1.04 . list_1 = [*range(0, 21,1)] list_1.sort(reverse=True) list_1 . list_1.reverse() list_1 . . list_2 = [ x**2 for x in list_1] list_2 . from math import log import random . list_1a = [random.randint(0,30) for x in range(0,100)] . sqrty = [randy**2 for randy in list_1a] . log_lst = [log(x + 1, 10) for x in sqrty] . Activity 1.01 Handling List . hundred_rand = [random.randint(0,30) for x in range(0,101)] . div_three = [x for x in hundred_rand if x % 3==0] . diff_len = len(hundred_rand) - len(div_three) . new_lst = [] number_of_experiment = 10 for g in range(0, number_of_experiment): randyx = [random.randint(0,100) for x in range(0,100)] div_3x = [x for x in randyx if x % 3==0] diff_len = len(randyx) - len(div_3x) new_lst.append(diff_len) new_lst . from scipy import mean . the_mean = mean(new_lst) the_mean . Introduction to Sets . list_12 = list(set(hundred_rand)) list_12 . Union and Intersection of Set . set_1 = {&#39;Apple&#39;, &#39;Orange&#39;, &#39;Banana&#39;} set_2 = {&#39;Pear&#39;, &#39;Peach&#39;, &#39;Mango&#39;, &#39;Banana&#39;} #the union of two set is .. set_1 | set_2 . set_1 &amp; set_2 . Creating Null set . non_set = set({}) non_set . Dictionary . dict_1 = {&#39;key1&#39;:&#39;value1&#39;, &#39;key2&#39;:&#39;value2&#39;} . Accessing and Setting Values in a dictionary . Revisiting the unique Valued List Problem . dict() fromkeys() and keys() | . list_rand = [random.randint(0,30) for x in range(0,100)] . list(dict.fromkeys(list_rand).keys()) . Deleting a Value From Dict Ex.1.09 . Involves deleting a value from dict using the del method . dict_1 = {&quot;key1&quot;: 1, &quot;key2&quot;: [&quot;list_element1&quot;, 34], &quot;key3&quot;: &quot;value3&quot;, &quot;key4&quot;: {&quot;subkey1&quot;: &quot;v1&quot;}, &quot;key5&quot;: 4.5} dict_1 . del dict_1[&#39;key2&#39;] dict_1 . del dict_1[&#39;key3&#39;] del dict_1[&#39;key4&#39;] dict_1 . Dictionary Comprehension ex 1.10 . Dictionary comprehension though rarely used but could come handle in the processing of creating important key-value pairs form of data like names of customer and their age, credit card customer and their owners . list_dict = [x for x in range(0,10)] dict_comp = {x: x**2 for x in list_dict} dict_comp . ## using the dict fuction dict_2 = dict([(&#39;Tom&#39;,100),(&#39;Dick&#39;,200),(&#39;Harry&#39;,300)]) dict_2 . dict_3 = dict(Tom=100, Dick=200,Harry=300) dict_3 . Tuples . A unique feature of tuple is that of immutability. That is once crested it can not be updated by way of adding or removing from it . | tuple consist of values separated by comma . | . tuple_1 = 24,42,2.3456, &#39;Hello&#39; . the length of the tuple is called cardinality | . Creating a Tuple with Different Cardinality . tuple_1 = () . tuple_1 = &#39;Hello&#39;, . tuple_1 = &#39;hello&#39;, &#39;there&#39; tuple_12 = tuple_1, 45, &#39;Sam&#39; tuple_12 . # tuple_1 = &#39;Hello&#39;, &#39;World!&#39; # tuple_1[1] = &#39;Universe&#39; . tuple_1 = (&#39;good&#39;,&#39;morning!&#39;, &#39;how&#39;,&#39;are&#39;,&#39;you?&#39;) tuple_1[0] . tuple_1[4] . Unpacking a Tuple . tuple_1 = &#39;Hello&#39;, &#39;World&#39; hello, world = tuple_1 print(hello) print(world) . Handling Tuple Ex 1.11 . tupleE = &#39;1&#39;, &#39;3&#39;, &#39;5&#39; tupleE . print(tupleE[0]) print(tupleE[1]) . Strings . An important feature of string is that it&#39;s immutable | . Accessing String Ex1.12 . string_1 = &quot;Hello World!&quot; string_1 . string_1[0] . string_1[4] . string_1[-1] . String Slices Ex 1.13 . string_a = &quot;Hello World! I am Learning data wrangling&quot; string_a . string_a[2:10] . string_a[-31:] . string_a[-10:-5] . String Functions . len(string_a) . ## use lower() and upper() methods str_1 = &quot;A COMPLETE UPER CASE STRING&quot; . str_1.lower() . str_1.upper() . ## use find method str_1 = &quot;A complicated string look like this&quot; . str_1.find(&#39;complicated&#39;) . str_1.find(&#39;hello&#39;) . ### use the replace method str_1 . str_1.replace(&#39;complicated&#39;, &#39;simple&#39;) . Splitting and Joining String Ex 1.14 . split and join methods | use str.split(separator) | use str.join(separator) | . ## using split str_1 = &quot;Name, age, Sex, Address&quot; list_1 = str_1.split(&#39;,&#39;) list_1 . s = &#39;|&#39; s.join(list_1) . Activity 1.02 Analyzing a Multi-line String and Generating the Unique Word Count . multiline_text= &quot;&quot;&quot;It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife. However little known the feelings or views of such a man may be on his first entering a neighbourhood, this truth is so well fixed in the minds of the surrounding families, that he is considered the rightful property of some one or other of their daughters. &quot;My dear Mr. Bennet,&quot; said his lady to him one day, &quot;have you heard that Netherfield Park is let at last?&quot; Mr. Bennet replied that he had not. &quot;But it is,&quot; returned she; &quot;for Mrs. Long has just been here, and she told me all about it.&quot; Mr. Bennet made no answer. &quot;Do you not want to know who has taken it?&quot; cried his wife impatiently. &quot;You want to tell me, and I have no objection to hearing it.&quot; This was invitation enough. &quot;Why, my dear, you must know, Mrs. Long says that Netherfield is taken by a young man of large fortune from the north of England; that he came down on Monday in a chaise and four to see the place, and was so much delighted with it, that he agreed with Mr. Morris immediately; that he is to take possession before Michaelmas, and some of his servants are to be in the house by the end of next week.&quot; &quot;What is his name?&quot;&quot;&quot; . multiline_text . type(multiline_text), len(multiline_text) . multiline = multiline_text.replace(&#39; n&#39;,&#39;&#39;).replace(&#39;?&#39;,&#39;&#39;).replace(&#39;.&#39;,&#39;&#39;).replace(&#39;;&#39;,&#39;&#39;).replace(&#39;,&#39;,&#39;&#39;).replace(&#39;&quot;&#39;,&#39; &#39;) multiline . list_word = multiline.split(&#39; &#39;) list_word . unique_lst = list(set(list_word)) unique_lst . unique_dict = dict.fromkeys(list_word) unique_dict . for x in list_word: if unique_dict[x] is None: unique_dict[x] = 1 else: unique_dict[x] += 1 unique_dict . top_words = sorted(unique_dict.items(), key=lambda x: x[1], reverse=True) top_words[:25] .",
            "url": "https://ajakaiye33.github.io/DataMusings/jupyter/2020/11/25/chap1-notes.html",
            "relUrl": "/jupyter/2020/11/25/chap1-notes.html",
            "date": " • Nov 25, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Readingdata",
            "content": "How Pandas’s read_excel Saved My Life . . Introduction . As common amongst the majority of individuals under the category of Data Scientist by Accident, once in a while, you come across what I call ‘moments of enlightenment’ which more or less upgrade your knowledge about the ‘tools of the trade’. I encountered one of such moments a couple of days ago. And since I just set up a new blog courtesy of fastai, I decided to relay my experience on how I entered Zen from discovering the usefulness and existence of pandas’s read_html methods! . Data in the Data Science Process . I make bold to say that the entire Data Science process revolves around one main resource: DATA. Data is an essential raw material in the manufacturing of Data Visualization Models. While drawing on the analogy to drive home my point about data and its importance in the data Science pipeline, I’m very much aware that we don’t literally manufacture anything rather a methodical transformation of inputs takes place. Thus raw data as an input is transformed into an output format that solves one problem or needs in the society. No doubt data is very important in the data Science process, but how do we go about getting this data into our data science work environment? . Getting Data into Jupyter Notebook . Before encountering Data Science, all I know about data and its handling was tied to one tool: MS EXCEL. Excel is an excellent tool in it own right but there come a time in your data handling experience when excel won’t just cut it. Most of my challenges with excel started manifesting when my data became larger. Lucky for me after narrating my challenges on stackoverflow and exposing myself to public ridicule from some not-too-kind responders, I went away with the knowledge of the existence of PANDAS and JUPYTER NOTEBOOK. Of course, with these new set of tools, I became the go-to-guy in the office for any data wrangling exercise and functions. Majority of the data I use comes from scraping them off websites pages and transforming the data into csv format which I then read into the jupyter notbook for onward processing, exploration and analysis. Aside from the pandas’s read_csv method I knew from reading the documentation I could use read_json, read_table, read_html e.t.c to load data unto jupyter notbook but never attempted using them until recently when I attended an online workshop, and the facilitator used the pandas’s read_html method. . ### Can Life be this fun and Easy? My excitement knew no bounds when it occurred to me that I can improve productivity levels by just reading and loading data directly from websites pages especially when the data I’m interested in are housed within html’s table tags. For example, to scrape this site normally, I will write the following code: . def get_site_html(url): try: html = requests.get(url) except HTTPError as e: return None try: bs = BeautifulSoup(html.content, &#39;html.parser&#39;) title = bs except AttributeError as e: return None return title def get_data(): site_html = get_site_html(&#39;https://covid19.ncdc.gov.ng/&#39;) the_table = site_html.find(&#39;table&#39;, {&#39;id&#39;: &#39;custom1&#39;}) head_table = the_table.thead.get_text().split(&#39; n&#39;)[2:6] # print(the_table) body_table = the_table.find_all(&#39;tr&#39;) table_head = body_table[0] # print(table_head) table_row = body_table[1:] # print(table_row) headings = [ht.get_text() for ht in table_head.find_all(&#39;th&#39;)] # print(headings) gey = [] all_rows = [table_row[i].find_all(&#39;td&#39;) for i in range(len(table_row))] for j in all_rows: one_row = [] for h in j: one_row.append(h.get_text().replace(&#39; n&#39;, &#39;&#39;)) gey.append(one_row) df = pd.DataFrame(data=gey, columns=headings) return df . As much as I suspect the above sample code code be more concise, with pandas read_html method I can achieve the same purpose with just these few lines: . import pandas as pd data = pd.read_html(&#39;https://covid19.ncdc.gov.ng/&#39;) data.head() . I bet you can’t get more concise than that. These few lines have saved me alot of time and boosted my productivity to the roof. Hope you learnt a thing or two from this post. Watch out for my next post. Bye! .",
            "url": "https://ajakaiye33.github.io/DataMusings/2020/09/28/readingData.html",
            "relUrl": "/2020/09/28/readingData.html",
            "date": " • Sep 28, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://ajakaiye33.github.io/DataMusings/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://ajakaiye33.github.io/DataMusings/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://ajakaiye33.github.io/DataMusings/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ajakaiye33.github.io/DataMusings/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}