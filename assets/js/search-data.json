{
  
    
        "post0": {
            "title": "2020:Learning Journey and Milestones",
            "content": ". Introduction . Hey there, yes you and everyone that have taken out time to read my blog or ‘blabbing page’ this year, I say thank you from the bottom of my heart! There is no doubt that this year, ‘the year of the corona-virus’, has been quite tough and challenging. Not only was our social-economic life disrupted, a lot of people, according to the World Health Organization, -over 1.7 million people died from the complications of the covid-19 virus. . Obviously, a greater part of the year was inundated with the gloom and doom of the coronavirus, but in the midst of these dark episodes, there were occasional flashes of light, warmth and hope here and there. I had my own flashes. Hope you had yours. Today’s post will be more or less a review of my struggles and achievements in the year of the coronavirus! . To do justice to this topic, I have decided to discuss my trumps and triumphs under the following subheadings: . Books read Technical Books | Non-Technical Books | . | Data Science Projects | Volunteer Work | Blog Posts | Certifications and Exams | . Certification and Exams . I’m proud to say that being certified as an AWS cloud Practioner was one of the positive moments of the year 2020. As a matter of fact, the certification was one of my first achievements of the year. I sat for the exam on the 23rd of January 2020 and after barely 24 hours received an email from AWS through Certmetrics saying: . You will agree with me that the timing between when I received notice of my results and the subsequent onslaught of the coronavirus was quite creepy and inauspicious because less than two weeks, the infamous total ‘shutdowns’ started owing to the spread of the covid19 virus. No thanks to covid-19, I never had the opportunity to celebrate and take myself out for a drink at the pubs. The experience that humanity was undergoing was depicted in a Pinterest Parody post which showed humans taking the place of the animals in the zoo and like a role reverse scenario, the animals roaming freely on our empty streets and parks! . As a survivalist strategy, I decided to put my hands and mind to work as a means of coping with the apparent boredom. To this end, I started getting my hands dirty with data science projects. . Data Science Projects . My first project was a personal attempt to make sense of my country’s data on the covid-19 virus. I started with recording and tracking on daily bases reports on the numbers of confirmed cases, deaths, hospitalizations and recoveries as I could not find a time series format data sets with these variables for the different states of the country. Fortunately, I was lucky to come across a website that had the data set I needed. So I wrote a small python script to extract and pipe the data to my app, which updates automatically with no manual intervention in any way. . I wanted to display my Exploratory Data Analysis, visualizations and predictive modelling skills to the World. This desires led me to streamlit platform. With streamlit, I was able to turn my jupyter notebook to a dashboard! I personally love streamlit, and I strongly suggest you try it out if you have not. . Through streamlit, I put together a dashboard with varied visualizations and also implemented with Prophet a forecasting model to predict the growth and plateau of confirmed covid-19 cases. To host my streamlit app so that it will be available to everyone in the world, I turned to Heroku. Heroku made deployment easy and intuitive! Although still under active building process you can see the deployed version of the app here . My second project was a capstone project for my diploma degree in Data Science and Analytics from the University of Petroleum Energy India. Of course, it was a virtual program which took around six months to complete. The live lectures were conducted via zoom and GoToMeeting apps. We were given five topics and dataset. The capstone is expected to be carried out on one area or topic. I chose the topic which required building a predictive model to detect and prevent energy theft, i.e. Electricity Theft. My mentor was a thorough and fantastic dude. With his guide, I was able to build two models: Regression model version and Classification model version. The most import take away from this project for me was the new skills I learned through my mentor on how to handle and detect outliers in machine learning projects. You can see the completed version of the project here . Books Read . Technical Books . I studied three books on data science, data handling and python. Python Projects For Beginners, by Connor P. Milliken I really love the ten weeks Journey this book took me through! The author structured the book in such a way that every day of the ten weeks is filled with one activity or another and at the end of the week, a mini-project covering contents and terms studied in prior weekdays is given to the student. I took away a lot of new approach to using the python language for data handling and visualizations. . Python Feature Engineering Cookbook by Soledad Galli This is one of my favourite read for the Year. Although I have read it cover to cover but often time I refer to it now and then. The book is a good read, especially if you are new to the craft of Feature Engineering. Above all, I like the author’s procedures which she documented as a veritable optional package to scikit-learn concerning feature engineering. . The Data Wrangling Workshop by *Brian Lipp et al * This is one of the best technical books I have studied in a long time. This book will blow your socks off if your desire is learning to handle messy data with python. It contains a ton of tricks and gems on data handling. . Non-Technical Books . You may be wondering after all the time on projects and technical books, how do I have time for ‘leisure reading’? You are probably right, my plate was quite full, but the drive to kill boredom and avoid my ‘mind turning into devils workshop’ turned me into a voracious reader. . Supper Thinking, The Big Book of Mental Models by Gabriel Weinberg This book is a classic. I have the ebook and audio versions. I actually was amongst the first few that purchased it when it came out. I bought it because I read a blog post done by the same author and was totally sold! I read it with so much intensity that I made out over 50 A4 size pages of notes and mind maps. It is a book that will turn you into a walking problem solver. After reading this book, I can say comfortably that the tendency to make an error judgement call on policies and issues, in general, has reduced to bearest minimum. . Technical Blog . This is one of the scariest things I did in the Year. Scarest because I faced my fears and demons. I read somewhere that one doesn’t truly know something until he teaches that thing to someone else. I started this blog as a way to document my thoughts processes as I learn the art and craft of Data Science and transition from Trade Analyst to a Data Scientist. I have always wanted to set up a blog, but the numerous hoops and obstacles a newbie was expected to jump and walk-around discouraged me. However, I overcame this hurdle when I encountered Mr Jeremy, one of the founders of fast ai platform. Jeremy and his fantastic team put together a fast pages project which magically turned normal jupyter notebook and medium file into a blog post with all the trappings of a normal website. This discovery was a big relief because all I needed was a code editor to write my medium files-just like this post, which I’m writing with atom’s editor or a jupyter notebook and a GitHub account. I push the files to GitHub, and voila I get back a beautiful laid out blog post!! . Volunteerism . This is one area that I foresee myself doing more in the coming year. Volunteerism is all about using my acquired skills to solve problems in the non-for-profit space and other data science-related organizations. I contributed my quota as a volunteer worker in this years DATA NATIVE CONFERENCE. Data Native is a media platform which showcases individuals and organizations doing something worthwhile in the data science space. I had a lot of fun, and I meet a lot of smart people in the field of Data Science and Data Engineering. I particularly learned the logistics and planning that goes into putting together a world-class Data Science Conference. I look forward to next years conference even though this year’s conference was virtual. . Conclusion . I hope I did not bore you with my shameless ‘self-gratifying’ stories of my struggles in the year of the coronavirus. I have read books built data science projects started my AWS certification journey, volunteered and applied my skill to solve real-life problems and started a blog post where I share with you my learning journey and wow moments. I hope you have learned something today. Bye!! .",
            "url": "https://ajakaiye33.github.io/DataMusings/markdown/2020/12/21/reflections.html",
            "relUrl": "/markdown/2020/12/21/reflections.html",
            "date": " • Dec 21, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Data Preprocessing:Pandas Vs Scikit-Learn",
            "content": ". Introduction . When I was a child, I talked like a child, I thought like a child, I reasoned like a child. When I became a man, I set aside childish ways - 1 Corinthians 13:11 . In a previous post, I posited that a key aspect of Data Science is the Data Science Process Life Circle. Essentially, DSLC is a bunch of activities and processes which underpinned the Data Science process. Some of the procedure which makes up the Data Science Process Life Circle are: . Obtain | Scrub | Explore | Model | Interpret | . Given the topic of this post our discussion shall revolve around just two out of the above five procedures: Scrub and Model Data scrubbing and Model Building are like siamese twins; the output of the former serves as the input for the latter. In this relationship, the relevance of the Data preprocessing stage can not be overemphasized in the subsequent activity of Model Building. As a matter of fact, the majority of Data Science practitioners holds the view that data preprocessing has a strong relationship with the model’s performance. . Therefore today’s post is centred around the tools for data preprocessing, their advantages and disadvantages, especially given model performance. . Typical Data Preprocessing Activities . I sensed some of us might be curious as to ‘what are typical examples of this data preprocessing’ stage. I’m happy to address such and similar curiosity. Data preprocessing is any activity or procedure carried out on a dataset necessary for building a model. Examples of typical data preprocessing activities are: . Detecting and handling missing values | Detecting and handling outliers | Creating new features from existing once | Drop existing features | transforming categorical features | transforming numerical features e.t.c. | . Tools for Data Preprocessing . The above-listed activities of data preprocessing can be carried out with the aid of two broad tools or modules: Pandas Preprocessing and Scikit-learn Preprocessing. . Preprocessing with Pandas . Pandas is one of the critical tools for data science in general. Its creation without missing words moved data science as a profession forward. As a matter of fact, my mentor who introduced me to the beautiful world of data Science believed all preprocessing procedures in machine learning must be carried out with pandas. This view though not totally correct made be embraced and believed pandas as the sole data preprocessing tool. However, you could summit that I had a narrow perspective concerning data preprocessing. Still, I’m thankful for such short-sightedness as it allowed me to deepen my knowledge and understanding of Pandas. some of the popular pandas methods and functions for data preprocessingg are as follows: . get_dummies() | fillna() | replace() | ffill() | bfill() e.t.c. | . A major drawback in using pandas for data preprocessing is data leakage. Thus with pandas, the chance for data leakage is higher when pandas are used for data preprocessing. This is possible because pandas don’t have the capability to preprocess and learn in one breathe like scikit-learn. . Preprocessing with Scikit-learn . Scikit-learn is a python package which houses the algorithms with which models are built. Scikit-learn module is principally used for performing machine learning and model building. However, I recently came across a blogpost which brought to my awareness the advantages of doing data preprocessing in scikit-learn as against pandas. In that blog post, the author argued that when you use scikit-learn to preprocess data: . You can cross-validate the entire workflow | You can grid search model and preprocess hyperparameters | Avoids adding new columns to the source DataFrame | Pandas lacks separate fit/transform steps to prevent data leakage. | Since I encountered scikit-learn for preprocessing, I can categorically state that not only has my workflow become more simplified my model performance has also improved tremendously. . Demonstrate scikit-learn for Data Preprocessing and model Building . Import necessary modules and packages . # import necessary package and classes import pandas as pd from sklearn.preprocessing import OneHotEncoder from sklearn.impute import SimpleImputer from sklearn.linear_model import LogisticRegression from sklearn.compose import make_column_transformer from sklearn.pipeline import make_pipeline . Load the Dataset and select rows and columns . # load dataset df = pd.read_csv(&#39;http://bit.ly/kaggletrain&#39;, nrows=6) cols = [&#39;Embarked&#39;, &#39;Sex&#39;, &#39;Age&#39;, &#39;Fare&#39;] X = df[cols] . Initialize classes . ohe = OneHotEncoder() #endcode 1 and 0 in place of categorical values imp = SimpleImputer() #replace missing values with mean clf = LogisticRegression() # our choice of algorithm for the model . Preprocess and Fit Model . ct = make_column_transformer( (ohe, [&#39;Embarked&#39;, &#39;Sex&#39;]), (imp, [&#39;Age&#39;]), remainder=&#39;passthrough&#39;)# ignore other columns while you preprocess the specified features/columns ) . Set step in to Pipeline . pipe = make_pipeline(ct, clf) . From the above snippet of code, we can see that we could carry out some preprocessing activities like replacement of missing values, dummy creation e.t.c in one fell swoop under one function:make_column_transformer . Conclusion . Machine learning tends to be complicated and complex very quickly. So it behoves the data science practitioner to avail himself of the right tools to reduce as much as a possible unnecessary complication. In the course of this post, we have discourse, demonstrated and drew our attention to the advantages of using python’s scikit-learn package for data preprocessing against python’s Pandas. I hope you have learned something today. Bye! .",
            "url": "https://ajakaiye33.github.io/DataMusings/markdown/2020/12/14/prepro-with-sklearn.html",
            "relUrl": "/markdown/2020/12/14/prepro-with-sklearn.html",
            "date": " • Dec 14, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Data Restructure",
            "content": ". Introduction . Some couple of weeks ago, I can remember I drew your attention to the bitter truth about Data Science. In that piece, I posited that beyond the &#39;sexiness&#39; and other euphemisms wrapped around the practice of Data Science, Data Science is Data wrangling and cleaning! How else can one describe a profession where over 90 per cent of its activities revolves around data cleaning and wrangling? . I&#39;m happy my mentors drilled this concept of Data Science into my subconscious mind at the very beginning of my encounter with Data Science. The view that Data Cleaning and wrangling form a more significant part of Data Science practice has not only humbled me but has helped me to develop a growth mindset in my quest to learn and practice Data Science. To this end, the majority of the data in the wild will need a significant amount of restructuring before commencing a more detailed analysis. . My attention this week is turned to the concept and practice of data restructuring or &#39;tidy data&#39;. Tidy data is one topic; in my opinion, critical to building a solid foundation in Data Science. Data restructure as a procedure has grown in lips and bounds so much so that in data science and analysis parlance, thanks to Hadley Wickham is now popularly referred to as Tidy Data . What is Tidy Data? . The concept of Tidy Data will be best understood when demonstrated than theoretically explained. To this end, in the course of this piece, I will use examples from real-life data to make the topic more interactive and possibly hands-on. . According to the father of Tidy Data, a dataset is messy if it does not meet the following guidelines: . Each variable from a column | Each observation from a row | Each type of observational unit forms a table | . Messy or Untidy Data . Since the taste of the pudding is in the eating, we will display an example of messy data in the real-world. Our dataset, for this post, is the record of students in private and public schools performance in the West African Examination Council (WAEC) Examination from the year 2016 to 2018. This is how the data set look like when viewed with MS Excel: . Types of Messy Data . The first step to resolving messy data is to recognize it when it exists. However, Hadley explicitly mentioned five of the most common types of dirty data: . Columns names are values, not variables names | Multiple variables are stored in column names | Variables are stored in both rows and columns | Multiple types of observational units are stored in the same table | A single observational unit is stored in multiple tables | . Tidy Data Procedure . Tidying data does not typically involve changing the value of a dataset, filling in missing values or doing any analysis. Tidying data consist of changing the shape or structure of the data to meet the tidy data principle . Read Data into Pandas . As customary, Pandas is the go-to tool for cleaning, restructuring, wrangling and visualizing data. Our goal, as far as this dataset is concerned, is to get for each year the private and public school performance of both sex and their respective states. Without doubt, to get our dataset from its present structure to the structure indicated above, we will perform some restructuring and tidying. Thus from the current dataset, we would extract the following variable/columns and variable values. . States | Year | Sex | Five_credit_and_above - Percentage of student that earned five credit and above | . import sys;sys.path.extend([r&quot;/Users/user/anaconda3/envs/wrangling_data/lib/python3.7/site-packages&quot;]) import pandas as pd import numpy as np import matplotlib.pyplot as plt messy_data_private_2016 = pd.read_excel(&#39;./The-Data-Wrangling-Workshop/Chapter09/datasets/waec_2016_2018.xlsx&#39;, sheet_name=&#39;private 2016&#39;) messy_data_public_2016 = pd.read_excel(&#39;./The-Data-Wrangling-Workshop/Chapter09/datasets/waec_2016_2018.xlsx&#39;, sheet_name=&#39;Public 2016&#39;) messy_data_private_2017 = pd.read_excel(&#39;./The-Data-Wrangling-Workshop/Chapter09/datasets/waec_2016_2018.xlsx&#39;, sheet_name=&#39;Private 2017&#39;) messy_data_public_2017 = pd.read_excel(&#39;./The-Data-Wrangling-Workshop/Chapter09/datasets/waec_2016_2018.xlsx&#39;, sheet_name=&#39;public 2017&#39;) messy_data_private_2018 = pd.read_excel(&#39;./The-Data-Wrangling-Workshop/Chapter09/datasets/waec_2016_2018.xlsx&#39;, sheet_name=&#39;Private 2018&#39;) messy_data_public_2018 = pd.read_excel(&#39;./The-Data-Wrangling-Workshop/Chapter09/datasets/waec_2016_2018.xlsx&#39;, sheet_name=&#39;Public 2018&#39;) . messy_data_private_2016.head() . WEST AFRICAN EXAMINATIONS COUNCIL Unnamed: 1 Unnamed: 2 Unnamed: 3 Unnamed: 4 Unnamed: 5 Unnamed: 6 Unnamed: 7 Unnamed: 8 Unnamed: 9 Unnamed: 10 Unnamed: 11 Unnamed: 12 Unnamed: 13 Unnamed: 14 Unnamed: 15 . 0 RESULT STATISTICS FOR 5 CREDITS IN ENGLISH, MA... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 WASSCE PRIVATE CANDIDATES EXAMINATION 2016 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 STATE | TOTAL NUMBER SAT | NaN | NaN | 5 CREDITS AND ABOVE INCLUDING ENGLISH LANGUAGE | NaN | NaN | 5 CREDITS AND ABOVE INCLUDING MATHEMATICS | NaN | NaN | 5 CREDITS &amp; ABOVE INCLUDING MATHEMATICS &amp; ENGL... | NaN | NaN | % OF 5 CREDITS AND ABOVE | NaN | NaN | . 3 NaN | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | . 4 ABIA | 309 | 276 | 585 | 65 | 59 | 124 | 39 | 27 | 66 | 35 | 25 | 60 | 11.3269 | 9.05797 | 10.2564 | . messy_data_public_2016.head() . WEST AFRICAN EXAMINATIONS COUNCIL Unnamed: 1 Unnamed: 2 Unnamed: 3 Unnamed: 4 Unnamed: 5 Unnamed: 6 Unnamed: 7 Unnamed: 8 Unnamed: 9 Unnamed: 10 Unnamed: 11 Unnamed: 12 Unnamed: 13 Unnamed: 14 Unnamed: 15 . 0 RESULT STATISTICS FOR 5 CREDITS IN ENGLISH, MA... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 WASSCE CANDIDATES EXAMINATION 2016 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 STATE | TOTAL NUMBER SAT | NaN | NaN | 5 CREDITS AND ABOVE INCLUDING ENGLISH LANGUAGE | NaN | NaN | 5 CREDITS AND ABOVE INCLUDING MATHEMATICS | NaN | NaN | 5 CREDITS &amp; ABOVE INCLUDING MATHEMATICS &amp; ENGL... | NaN | NaN | % OF 5 CREDITS &amp; ABOVE INCLUDING MATHEMATICS &amp;... | NaN | NaN | . 3 NaN | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | . 4 ABIA | 24322 | 27405 | 51727 | 20403 | 23564 | 43967 | 18485 | 20688 | 39173 | 17264 | 19526 | 36790 | 70.981 | 71.2498 | 71.1234 | . messy_data_private_2017.head() . WEST AFRICAN EXAMINATIONS COUNCIL Unnamed: 1 Unnamed: 2 Unnamed: 3 Unnamed: 4 Unnamed: 5 Unnamed: 6 Unnamed: 7 Unnamed: 8 Unnamed: 9 Unnamed: 10 Unnamed: 11 Unnamed: 12 Unnamed: 13 Unnamed: 14 Unnamed: 15 . 0 RESULT STATISTICS FOR 5 CREDITS IN ENGLISH ,M... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 WASSCE PRIVATE CANDIDATES EXAMINATION 2017 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 STATE | TOTAL NUMBER SAT | NaN | NaN | 5 CREDITS AND ABOVE INCLUDING ENGLISH LANGUAGE | NaN | NaN | 5 CREDITS AND ABOVE INCLUDING MATHEMATICS | NaN | NaN | 5 CREDITS &amp; ABOVE INCLUDING MATHEMATICS &amp; ENGL... | NaN | NaN | % OF 5 CREDITS &amp; ABOVE INCLUDING MATHEMATICS &amp;... | NaN | NaN | . 3 NaN | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | . 4 ABIA | 331 | 279 | 610 | 128 | 116 | 244 | 106 | 105 | 211 | 96 | 99 | 195 | 29.003 | 35.4839 | 31.9672 | . messy_data_public_2017.head() . WEST AFRICAN EXAMINATIONS COUNCIL Unnamed: 1 Unnamed: 2 Unnamed: 3 Unnamed: 4 Unnamed: 5 Unnamed: 6 Unnamed: 7 Unnamed: 8 Unnamed: 9 Unnamed: 10 Unnamed: 11 Unnamed: 12 Unnamed: 13 Unnamed: 14 Unnamed: 15 . 0 RESULT STATISTICS FOR 5 CREDITS IN ENGLISH ,MA... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 WASSCE CANDIDATES EXAMINATION 2017 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 STATE | TOTAL NUMBER SAT | NaN | NaN | 5 CREDITS AND ABOVE INCLUDING ENGLISH LANGUAGE | NaN | NaN | 5 CREDITS AND ABOVE INCLUDING MATHEMATICS | NaN | NaN | 5 CREDITS &amp; ABOVE INCLUDING MATHEMATICS &amp; ENGL... | NaN | NaN | % OF 5 CREDITS &amp; ABOVE INCLUDING MATHEMATICS &amp;... | NaN | NaN | . 3 NaN | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | . 4 ABIA | 23649 | 26023 | 49672 | 19317 | 21712 | 41029 | 20727 | 22931 | 43658 | 17928 | 20084 | 38012 | 75.8087 | 77.1779 | 76.526 | . messy_data_private_2018.head() . WEST AFRICAN EXAMINATIONS COUNCIL Unnamed: 1 Unnamed: 2 Unnamed: 3 Unnamed: 4 Unnamed: 5 Unnamed: 6 Unnamed: 7 Unnamed: 8 Unnamed: 9 ... Unnamed: 16374 Unnamed: 16375 Unnamed: 16376 Unnamed: 16377 Unnamed: 16378 Unnamed: 16379 Unnamed: 16380 Unnamed: 16381 Unnamed: 16382 Unnamed: 16383 . 0 RESULT STATISTICS FOR 5 CREDITS IN ENGLISH ,M... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 WASSCE PRIVATE CANDIDATES EXAMINATION 2018 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 STATE | TOTAL NUMBER SAT | NaN | NaN | 5 CREDITS AND ABOVE INCLUDING ENGLISH LANGUAGE | NaN | NaN | 5 CREDITS AND ABOVE INCLUDING MATHEMATICS | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 3 NaN | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 4 ABIA | 308 | 291 | 599 | 176 | 176 | 352 | 172 | 170 | 342 | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 5 rows × 16384 columns . messy_data_public_2018.head() . WEST AFRICAN EXAMINATIONS COUNCIL Unnamed: 1 Unnamed: 2 Unnamed: 3 Unnamed: 4 Unnamed: 5 Unnamed: 6 Unnamed: 7 Unnamed: 8 Unnamed: 9 Unnamed: 10 Unnamed: 11 Unnamed: 12 Unnamed: 13 Unnamed: 14 Unnamed: 15 . 0 RESULT STATISTICS FOR 5 CREDITS IN ENGLISH ,M... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 WASSCE CANDIDATES EXAMINATION 2018 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 STATE | TOTAL NUMBER SAT | NaN | NaN | 5 CREDITS AND ABOVE INCLUDING ENGLISH LANGUAGE | NaN | NaN | 5 CREDITS AND ABOVE INCLUDING MATHEMATICS | NaN | NaN | 5 CREDITS &amp; ABOVE INCLUDING MATHEMATICS &amp; ENGL... | NaN | NaN | % of 5 CREDITS AND ABOVE INCLUDING MATHEMATICS... | NaN | NaN | . 3 NaN | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | MALE | FEMALE | TOTAL | . 4 ABIA | 22502 | 24966 | 47468 | 19711 | 22056 | 41767 | 20239 | 22426 | 42665 | 18484 | 20572 | 39056 | 82.1438 | 82.4001 | 82.2786 | . Remove/skip Useless Values and Variables . From the above MS Excel display of the data we can see that we need to skip some rows of values that are not useful for our desired data structure . skip_rows_private_2016 = pd.read_excel(&#39;./The-Data-Wrangling-Workshop/Chapter09/datasets/waec_2016_2018.xlsx&#39;, sheet_name=&#39;private 2016&#39;,header=None,skiprows=[0,1,2]) rename_col = skip_rows_private_2016.iloc[2:39,[0, 13,14]] rename_col = rename_col.rename(columns={0:&#39;States&#39;, 13:&#39;Male&#39;,14:&#39;Female&#39;}) rename_col[&#39;School_type&#39;] = &#39;private&#39; rename_col[&#39;Year&#39;] = 2016 #public 2016 skip_rows_public_2016 = pd.read_excel(&#39;./The-Data-Wrangling-Workshop/Chapter09/datasets/waec_2016_2018.xlsx&#39;, sheet_name=&#39;Public 2016&#39;,header=None, skiprows=[0,1,2]) rename_col_2016_b = skip_rows_public_2016.iloc[2:39,[0,13,14]] rename_col_2016_b = rename_col_2016_b.rename(columns={0:&#39;States&#39;, 13:&#39;Male&#39;, 14:&#39;Female&#39;}) rename_col_2016_b[&#39;School_type&#39;] = &#39;public&#39; rename_col_2016_b[&#39;Year&#39;] = 2016 #private 2017 skip_rows_private_2017 = pd.read_excel(&#39;./The-Data-Wrangling-Workshop/Chapter09/datasets/waec_2016_2018.xlsx&#39;, sheet_name=&#39;Private 2017&#39;,header=None,skiprows=[0,1,2]) rename_col_2017 = skip_rows_private_2017.iloc[2:39,[0, 13,14]] rename_col_2017 = rename_col_2017.rename(columns={0:&#39;States&#39;, 13:&#39;Male&#39;,14:&#39;Female&#39;}) rename_col_2017[&#39;School_type&#39;] = &#39;private&#39; rename_col_2017[&#39;Year&#39;] = 2017 #public 2017 skip_rows_public_2017 = pd.read_excel(&#39;./The-Data-Wrangling-Workshop/Chapter09/datasets/waec_2016_2018.xlsx&#39;, sheet_name=&#39;public 2017&#39;,header=None, skiprows=[0,1,2]) rename_col_2017_public = skip_rows_public_2017.iloc[2:39,[0,13,14]] rename_col_2017_public = rename_col_2017_public.rename(columns={0:&#39;States&#39;, 13:&#39;Male&#39;, 14:&#39;Female&#39;}) rename_col_2017_public[&#39;School_type&#39;] = &#39;public&#39; rename_col_2017_public[&#39;Year&#39;] = 2017 #private 2018 skip_rows_private_2018 = pd.read_excel(&#39;./The-Data-Wrangling-Workshop/Chapter09/datasets/waec_2016_2018.xlsx&#39;, sheet_name=&#39;Private 2018&#39;,header=None,skiprows=[0,1,2]) rename_col_2018_private = skip_rows_private_2018.iloc[2:39,[0, 13,14]] rename_col_2018_private = rename_col_2018_private.rename(columns={0:&#39;States&#39;, 13:&#39;Male&#39;,14:&#39;Female&#39;}) rename_col_2018_private[&#39;School_type&#39;] = &#39;private&#39; rename_col_2018_private[&#39;Year&#39;] = 2018 #public 2018 skip_rows_public_2018 = pd.read_excel(&#39;./The-Data-Wrangling-Workshop/Chapter09/datasets/waec_2016_2018.xlsx&#39;, sheet_name=&#39;Public 2018&#39;,header=None, skiprows=[0,1,2]) rename_col_2018_public = skip_rows_public_2018.iloc[2:39,[0,13,14]] rename_col_2018_public = rename_col_2018_public.rename(columns={0:&#39;States&#39;, 13:&#39;Male&#39;, 14:&#39;Female&#39;}) rename_col_2018_public[&#39;School_type&#39;] = &#39;public&#39; rename_col_2018_public[&#39;Year&#39;] = 2018 . Transform to Tidy Data . From the above current version of our dataset, it is evident that we have made significant progress. We have a data-frame of the percentages of males and females with their respective states that earned five credit and above. Going by the tidy data guideline, our data-frame need to undergo some structure adjustment and re-alignment. Since our goal is to merge all the data in all the sheet into one table, i.e. data-frame, other sheets would pass through similar procedures. . tidy_private_2016 = rename_col.melt(id_vars=[&#39;States&#39;], value_vars=[&#39;Male&#39;, &#39;Female&#39;],var_name=&#39;Sex&#39;,value_name=&#39;pct_five_credit_above&#39;) . tidy_private_2016 . States Sex pct_five_credit_above . 0 ABIA | Male | 11.3269 | . 1 ABUJA | Male | 6.53595 | . 2 ADAMAWA | Male | 6.42857 | . 3 AKWA IBOM | Male | 12.0308 | . 4 ANAMBRA | Male | 16.7679 | . ... ... | ... | ... | . 69 RIVERS | Female | 47.2096 | . 70 SOKOTO | Female | 12.9032 | . 71 TARABA | Female | 0 | . 72 YOBE | Female | 0 | . 73 ZAMFARA | Female | 0 | . 74 rows × 3 columns . Conclusion . From the above version of our dataset, we can see that the current version satisfies all the tidy data guidelines and principles. Thus our dataset is transformed from wide to long. We have in the course of this post demonstrated data cleaning, wrangling and restructuring. However, more activities can be still possible to clean and restructure to fine-tune the dataset further. Nevertheless, we would conclude this post here as we have achieved the objective of demonstrating how data tidying using pandas. I hope you have learned something today. .",
            "url": "https://ajakaiye33.github.io/DataMusings/fastpages/jupyter/2020/12/07/tidy.html",
            "relUrl": "/fastpages/jupyter/2020/12/07/tidy.html",
            "date": " • Dec 7, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Feature Selection In The Machine Learning Process",
            "content": "Introduction . Recently I had the rare privilege to encounter a brilliant and concise perspective of machine learning from an equally experienced practitioner in the field. According to my mentor, machine learning is the &quot;semi-automatic process of extracting information from Data&quot;. I have seen similar definitions and perspectives of the concept of machine learning, as there is no universally accepted definition yet. However, this particular definition caught my ears and attention because of the use of semi-automatic phrase! Understandably I was equally curious about the meaning of semi-automatic in the context of the machine learning process. . As a way of summary, machine learning is semi-automatic because the process involves two parts: qualitative and quantitative. The qualitative activities are those activities we often refer to as pre-modelling activities. More or less those activities that at best falls under the realm of &#39;art&#39; of data science, subjectively carried out to pre-process and shape the data in a manner amenable to a model, the quantitative aspect of the machine learning process. Some of these qualitative processes, like &#39;Feature Engineering&#39;, creates another problem of high dimensionality, which invariably leads to increased complexity,over-fitting and consequently poor performance of the model. . There are a plethora of ways devised to handle the problem of the high dimensionality of data. Feature selection is one of such useful methods. Feature selection, simply put, is a process of pruning the features or variables of a dataset to the &#39;necessary few&#39; that have high explained ratios. You can do Feature selection in three ways: . Univariate Statistics Process | Iterative Process (Recursive Feature Elimination-RFE) | Model-Based Feature Selection Process | . Given our sub-topic, we shall dwell on the Model-driven Process of Feature Selection in this post. . The Model Process of Feature Selection . Making feature selection with the aid of algorithms is one of the most straightforward and direct processes of feature selection. I must stress here that while it is one of the convenient ways of feature selection, not all algorithms are suited for it. In essence, Model-driven feature selection leaves the decision process of feature selection and de-selection to the whims and caprices of tree-base algorithms. . Model-driven feature selection is done in the following simple steps. . Have your Dataset at hand | split your dataset into training and test sets | Initialized your model | Fit the model with the training set of your dataset | Extract the important features | Adjust dataset according and proceed to the model building stage | . Model -Driven Feature Selection - Demonstration . We need dataset and algorithm to demonstrate the model-based feature selection effectively. To this end, we shall look to the versatile and comprehensive sci-kit learn python package. . Import the &#39;Select from model&#39; class and other necessary modules . import sys;sys.path.extend([r&quot;/Users/user/anaconda3/envs/wrangling_data/lib/python3.7/site-packages&quot;]) . from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split from sklearn.feature_selection import SelectFromModel from sklearn.ensemble import RandomForestClassifier import numpy as np import matplotlib.pyplot as plt cancer = load_breast_cancer() . cancer.data.shape . (569, 30) . make_up_num = np.random.RandomState(42) noise = make_up_num.normal(size=(len(cancer.data),50)) #first thirty features are from the data whereas the next 50 features are noise stak_noise = np.hstack([cancer.data, noise]) . X_train,X_test, y_train, y_test = train_test_split(stak_noise, cancer.target, random_state=42, test_size=.50) . cancer.feature_names . array([&#39;mean radius&#39;, &#39;mean texture&#39;, &#39;mean perimeter&#39;, &#39;mean area&#39;, &#39;mean smoothness&#39;, &#39;mean compactness&#39;, &#39;mean concavity&#39;, &#39;mean concave points&#39;, &#39;mean symmetry&#39;, &#39;mean fractal dimension&#39;, &#39;radius error&#39;, &#39;texture error&#39;, &#39;perimeter error&#39;, &#39;area error&#39;, &#39;smoothness error&#39;, &#39;compactness error&#39;, &#39;concavity error&#39;, &#39;concave points error&#39;, &#39;symmetry error&#39;, &#39;fractal dimension error&#39;, &#39;worst radius&#39;, &#39;worst texture&#39;, &#39;worst perimeter&#39;, &#39;worst area&#39;, &#39;worst smoothness&#39;, &#39;worst compactness&#39;, &#39;worst concavity&#39;, &#39;worst concave points&#39;, &#39;worst symmetry&#39;, &#39;worst fractal dimension&#39;], dtype=&#39;&lt;U23&#39;) . select = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42)) #The select for model class select all features that have an important measureof tge features #(as provided by the supervised model greater thanthe provided threshold) . select.fit(X_train, y_train) trans_xtrain = select.transform(X_train) print(f&#39;X_train.shape:{X_train.shape}&#39;) print(f&#39;trans_xtrain.shape:{trans_xtrain.shape}&#39;) . X_train.shape:(284, 80) trans_xtrain.shape:(284, 20) . Model Discard useless and irrelevant Features . From the above operation, our original data had 30 features. We generated additional 50 fake features to test the efficacy of the process. From our above result, upon transforming the training set, the number of features drastically reduced to 20 attributes. From the preceding process, the model didn&#39;t only do away with the 50 fictitious features it equally discarded 10 features from our original 30 features! . mask = select.get_support() #Visualize the mask: black is True and white is False plt.matshow(mask.reshape(1,-1), cmap=&#39;gray_r&#39;) plt.xlabel(&#39;Sample index&#39;) plt.yticks(()) . ([], []) . Plot feature important from trained model . Another way is to use the important feature method to visualize the importance of features. From the bellow bar graph we can see that the most important feature is the worst concave points whereas the least importance feature is the Symmetry error? . cancer = load_breast_cancer() X_train,X_test,y_train,y_test = train_test_split(cancer.data, cancer.target,random_state=42) forest = RandomForestClassifier(n_estimators=100, random_state=42) forest.fit(X_train,y_train) . RandomForestClassifier(random_state=42) . forest.feature_importances_ . array([0.03971058, 0.01460399, 0.05314639, 0.04277978, 0.00816485, 0.01140166, 0.08321459, 0.0902992 , 0.00443533, 0.00443395, 0.01951684, 0.00459978, 0.00868228, 0.04355077, 0.00464415, 0.0036549 , 0.00701442, 0.00504716, 0.00371411, 0.00658253, 0.08127686, 0.01649014, 0.07138828, 0.12319232, 0.01033481, 0.01580059, 0.03174022, 0.17229521, 0.01310266, 0.00518165]) . def plot_feature_importance_cancer(model): n_features = cancer.data.shape[1] fig, ax = plt.subplots(figsize=(12,8)) plt.barh(range(n_features),model.feature_importances_, align=&#39;center&#39;) plt.yticks(np.arange(n_features),cancer.feature_names) plt.xlabel(&#39;Feature Importance&#39;) plt.ylabel(&#39;Feature&#39;) . plot_feature_importance_cancer(forest) . Conclusion . In the course of this post, we emphasized the relevance of feature importance in the machine learning process. Feature selection is such a necessary process that I make bold to say any good model worth its salt must have undergone feature selection in one way or another. We can also appreciate the ease of using a model to effect feature selection. I hope you have learned something today. Bye! .",
            "url": "https://ajakaiye33.github.io/DataMusings/jupyter/2020/11/30/Feature-Selection.html",
            "relUrl": "/jupyter/2020/11/30/Feature-Selection.html",
            "date": " • Nov 30, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Data Wrangling Fundamentals",
            "content": "Within the data science ecosystem, there is an overwhelming consensus that over ninety-five per cent of the data science process is data cleaning and wrangling. For such an important aspect of data science, you would expect greater attention, but the reality is that like human nature, we are almost always attracted to the shining parts of things. Consequently, beginners from the very beginning of their sojourn towards the unknown are lead astray. Do not be deceived; data science is data cleaning and data wrangling! As a veritable means of closing the yarning knowledge gap in the data wrangling and cleaning craft, I have decided to extract data-wrangling exercises from different resources. Thus this notebook and subsequent ones would highlight these exercises as well as their real-life application in the field of data science. . import numpy as np import pandas as pd . a = np.random.randn(5,3) a . array([[-1.12506181, -0.32504384, 1.67238151], [-0.04049187, -1.85380366, -1.47251248], [-1.39307679, -2.52397974, 0.96097851], [ 0.74104312, -0.14694375, -0.25140089], [-1.2898671 , -0.72692951, -1.49530227]]) . Steps of Data Wrangling . Scraping raw data from multiple sources(including web and database table) | Imputing(replacing missing data using various techniques), formating, and transforming-basically making it ready to be used in the modeling process | Handling read/write errors | Detecting outliers | Performing quick visualizations(plotting) and basic statistical analyses to judge the quality of formatted data | . Accessing The List Members Exercise:1.01 . ssn = list(pd.read_csv(&#39;./The-Data-Wrangling-Workshop/chapter01/datasets/ssn.csv&#39;)) . ssn[0] . ssn[3] . ssn[len(ssn)- 1] . ssn[-1] . ssn[:3] . ssn[-2:] . ssn[:-2] . ssn[-1::-1] . ssn[:] . Generating and Iterating through a List: 1.02 . ssn_2 = [] for i in ssn: ssn_2.append(i) ssn_2 . ssn3 = [&#39;soc: &#39;+ x for x in ssn_2] ssn3 . i = 0 while i &lt; len(ssn3): print(ssn3[i]) i += 1 . numbers = [x for x in ssn3 if &#39;5&#39; in x] numbers . ssn_4 = [&#39;102-90-0314&#39;,&#39;247-17-2338&#39;,&#39;318-22-2760&#39;] ssn_5 = ssn_4 + ssn ssn_5 . ssn_2.extend(ssn_4) ssn_2 . # for x in ssn_2: # for y in ssn_5: # print(str(x) + &#39;,&#39; + str(y)) . Iterating Over a List and Checking Membership: 1.03 . car_model = list(pd.read_csv(&#39;./The-Data-Wrangling-Workshop/chapter01/datasets/car_models.csv&#39;)) car_model . list_1 = [x for x in car_model] for i in range(0,len(list_1)): print(list_1[i]) . for i in list_1: print(i) . &#39;D150&#39; in list_1, &#39;Mustang&#39; in list_1 . Sorting A List: Exercise 1.04 . list_1 = [*range(0, 21,1)] list_1.sort(reverse=True) list_1 . list_1.reverse() list_1 . . list_2 = [ x**2 for x in list_1] list_2 . from math import log import random . list_1a = [random.randint(0,30) for x in range(0,100)] . sqrty = [randy**2 for randy in list_1a] . log_lst = [log(x + 1, 10) for x in sqrty] . Activity 1.01 Handling List . hundred_rand = [random.randint(0,30) for x in range(0,101)] . div_three = [x for x in hundred_rand if x % 3==0] . diff_len = len(hundred_rand) - len(div_three) . new_lst = [] number_of_experiment = 10 for g in range(0, number_of_experiment): randyx = [random.randint(0,100) for x in range(0,100)] div_3x = [x for x in randyx if x % 3==0] diff_len = len(randyx) - len(div_3x) new_lst.append(diff_len) new_lst . from scipy import mean . the_mean = mean(new_lst) the_mean . Introduction to Sets . list_12 = list(set(hundred_rand)) list_12 . Union and Intersection of Set . set_1 = {&#39;Apple&#39;, &#39;Orange&#39;, &#39;Banana&#39;} set_2 = {&#39;Pear&#39;, &#39;Peach&#39;, &#39;Mango&#39;, &#39;Banana&#39;} #the union of two set is .. set_1 | set_2 . set_1 &amp; set_2 . Creating Null set . non_set = set({}) non_set . Dictionary . dict_1 = {&#39;key1&#39;:&#39;value1&#39;, &#39;key2&#39;:&#39;value2&#39;} . Accessing and Setting Values in a dictionary . Revisiting the unique Valued List Problem . dict() fromkeys() and keys() | . list_rand = [random.randint(0,30) for x in range(0,100)] . list(dict.fromkeys(list_rand).keys()) . Deleting a Value From Dict Ex.1.09 . Involves deleting a value from dict using the del method . dict_1 = {&quot;key1&quot;: 1, &quot;key2&quot;: [&quot;list_element1&quot;, 34], &quot;key3&quot;: &quot;value3&quot;, &quot;key4&quot;: {&quot;subkey1&quot;: &quot;v1&quot;}, &quot;key5&quot;: 4.5} dict_1 . del dict_1[&#39;key2&#39;] dict_1 . del dict_1[&#39;key3&#39;] del dict_1[&#39;key4&#39;] dict_1 . Dictionary Comprehension ex 1.10 . Dictionary comprehension though rarely used but could come handle in the processing of creating important key-value pairs form of data like names of customer and their age, credit card customer and their owners . list_dict = [x for x in range(0,10)] dict_comp = {x: x**2 for x in list_dict} dict_comp . ## using the dict fuction dict_2 = dict([(&#39;Tom&#39;,100),(&#39;Dick&#39;,200),(&#39;Harry&#39;,300)]) dict_2 . dict_3 = dict(Tom=100, Dick=200,Harry=300) dict_3 . Tuples . A unique feature of tuple is that of immutability. That is once crested it can not be updated by way of adding or removing from it . | tuple consist of values separated by comma . | . tuple_1 = 24,42,2.3456, &#39;Hello&#39; . the length of the tuple is called cardinality | . Creating a Tuple with Different Cardinality . tuple_1 = () . tuple_1 = &#39;Hello&#39;, . tuple_1 = &#39;hello&#39;, &#39;there&#39; tuple_12 = tuple_1, 45, &#39;Sam&#39; tuple_12 . # tuple_1 = &#39;Hello&#39;, &#39;World!&#39; # tuple_1[1] = &#39;Universe&#39; . tuple_1 = (&#39;good&#39;,&#39;morning!&#39;, &#39;how&#39;,&#39;are&#39;,&#39;you?&#39;) tuple_1[0] . tuple_1[4] . Unpacking a Tuple . tuple_1 = &#39;Hello&#39;, &#39;World&#39; hello, world = tuple_1 print(hello) print(world) . Handling Tuple Ex 1.11 . tupleE = &#39;1&#39;, &#39;3&#39;, &#39;5&#39; tupleE . print(tupleE[0]) print(tupleE[1]) . Strings . An important feature of string is that it&#39;s immutable | . Accessing String Ex1.12 . string_1 = &quot;Hello World!&quot; string_1 . string_1[0] . string_1[4] . string_1[-1] . String Slices Ex 1.13 . string_a = &quot;Hello World! I am Learning data wrangling&quot; string_a . string_a[2:10] . string_a[-31:] . string_a[-10:-5] . String Functions . len(string_a) . ## use lower() and upper() methods str_1 = &quot;A COMPLETE UPER CASE STRING&quot; . str_1.lower() . str_1.upper() . ## use find method str_1 = &quot;A complicated string look like this&quot; . str_1.find(&#39;complicated&#39;) . str_1.find(&#39;hello&#39;) . ### use the replace method str_1 . str_1.replace(&#39;complicated&#39;, &#39;simple&#39;) . Splitting and Joining String Ex 1.14 . split and join methods | use str.split(separator) | use str.join(separator) | . ## using split str_1 = &quot;Name, age, Sex, Address&quot; list_1 = str_1.split(&#39;,&#39;) list_1 . s = &#39;|&#39; s.join(list_1) . Activity 1.02 Analyzing a Multi-line String and Generating the Unique Word Count . multiline_text= &quot;&quot;&quot;It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife. However little known the feelings or views of such a man may be on his first entering a neighbourhood, this truth is so well fixed in the minds of the surrounding families, that he is considered the rightful property of some one or other of their daughters. &quot;My dear Mr. Bennet,&quot; said his lady to him one day, &quot;have you heard that Netherfield Park is let at last?&quot; Mr. Bennet replied that he had not. &quot;But it is,&quot; returned she; &quot;for Mrs. Long has just been here, and she told me all about it.&quot; Mr. Bennet made no answer. &quot;Do you not want to know who has taken it?&quot; cried his wife impatiently. &quot;You want to tell me, and I have no objection to hearing it.&quot; This was invitation enough. &quot;Why, my dear, you must know, Mrs. Long says that Netherfield is taken by a young man of large fortune from the north of England; that he came down on Monday in a chaise and four to see the place, and was so much delighted with it, that he agreed with Mr. Morris immediately; that he is to take possession before Michaelmas, and some of his servants are to be in the house by the end of next week.&quot; &quot;What is his name?&quot;&quot;&quot; . multiline_text . type(multiline_text), len(multiline_text) . multiline = multiline_text.replace(&#39; n&#39;,&#39;&#39;).replace(&#39;?&#39;,&#39;&#39;).replace(&#39;.&#39;,&#39;&#39;).replace(&#39;;&#39;,&#39;&#39;).replace(&#39;,&#39;,&#39;&#39;).replace(&#39;&quot;&#39;,&#39; &#39;) multiline . list_word = multiline.split(&#39; &#39;) list_word . unique_lst = list(set(list_word)) unique_lst . unique_dict = dict.fromkeys(list_word) unique_dict . for x in list_word: if unique_dict[x] is None: unique_dict[x] = 1 else: unique_dict[x] += 1 unique_dict . top_words = sorted(unique_dict.items(), key=lambda x: x[1], reverse=True) top_words[:25] .",
            "url": "https://ajakaiye33.github.io/DataMusings/jupyter/2020/11/25/chap1-notes.html",
            "relUrl": "/jupyter/2020/11/25/chap1-notes.html",
            "date": " • Nov 25, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Practical Implementation of PCA",
            "content": "Introduction . The first time I encountered PCA(Principal Component Analysis) as a concept was in a Thinkful’s Introduction to Data Science Course. As much as I can remember I was baffled and bewildered. For weeks I couldn’t make head or tail of what it means and the ‘job’ it is meant to do in machine learning and data science. However, a considerable milestone was attained when my mentor concurred with my suggestion that it is similar to the Pareto Principle or the law of the vital few. The law of the vital few states that ‘for many outcomes, roughly 80% of consequences come from 20% of the causes’. In other words, 80% of the components of a subject lies in 20% of such components. . When ‘PP’ is juxtaposed with Principal Component Analysis, the problem is then reframed as “how to reduce the component that makes up a subject to as few components as possible without losing the meaning of such a subject”. Thus Principal Component Analysis from the perspective of machine learning and Data Science is a complexity-reduction technique that tries to reduce a set of variables down to a smaller set of components that represent most of the information in the variables. Through this transformation, a large chunk of the information across the full dataset is effectively compressed in fewer feature columns. It is the most popular Dimensionality Reduction Algorithm used in various operations like: . Noise Filtering | Feature extraction | visualization | Gene Data analysis | Stock market prediction e.t.c | . Keeping with the spirit of this blog, I shall not bore you with unnecessary mathematical theories/jargon about PCA; we shall concentrate on how I often apply it in model building exercise. However, you can read more about the mathematical foundation of PCA. There is A Tutorial on Principal Component Analysis by Jonathan Shlens at the Salk Institute. . To this end, in this post, I shall cover PCA in the following sub-categories/steps: . Initialization, Standardization &amp; Transformation | Explained Component Summary Report | Decision: How many components? | . For effective coverage we shall demonstrate with simple dataset . Initialization &amp; Standardization &amp; Transformation . #import necessary package and Modules import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA . To perform a PCA operation effectively the dataset must be Standardized and an object must be created from the PCA class . #read data unto pandas mydata = pd.read_csv(&#39;https://raw.githubusercontent.com/Thinkful-Ed/data-201-resources/master/ESS_practice_data/ESSdata_Thinkful.csv&#39;) mydata.head() . . For brevity we shall select four features only which is more than enough to demonstrate. . mydata.columns . . mydata[[&#39;tvtot&#39;, &#39;ppltrst&#39;,&#39;pplfair&#39;,&#39;pplhlp&#39;]] mydata.head() . . #Standardized data and place result into a pandas DataFrame mydata_std = pd.DataFrame(StandardScaler().fit_transform(mydata), columns= mydata.columns) #initte PCA pca = PCA(n_components=4) #get pcs version of data mydata_pca = pd.DataFrame(pca.fit_transform(mydata_std), columns=mydata.columns) mydata_std.head() mydata_pca.head() . . . Explained Component Summary Report . As a convention, normally I produce a report which makes the PCA operations easy to digest. And we do this with the three methods: . explained variance: These are the variance components accounted for by each principal component, in descending order. The sum of these variance components is the total variance. | explained variance ratio: These are the proportion of total data variance accounted for by each principal component, in descending order. They sum to 1.0 | Singular values: These are used to calculate the variance components. | . pca.explained_variance_ pca.explained_variance_ratio_ pca.singular_values_ . . # summary Report data = { &#39;Singular Value&#39;:pca.singular_values_, &#39;Variance&#39;:pca.explained_variance_, &#39;%Variance&#39;:pca.explained_variance_ratio_*100 } var_report = pd.DataFrame( data ) var_report[ &#39;Cum Sum&#39; ] = var_report[ &#39;%Variance&#39; ].cumsum() print( &#39;Number of observations: n = {}&#39;.format( pca.n_samples_ ) ) var_report . . The Variance is the contribution to the total variance of the data and is based on the singular values. Thus the sum of the Variance contribution is the total variance. The first principal component accounts for 51.5% of the variance and the first two account for 75.8%. . Decision: How many components? . The biggest decision to make when running a PCA is how many components to keep. There are several rules to guide us in choosing the number of the component to keep. The most straightforward is to keep components with eigenvalues or where Singular value is greater than 1 as these components add value. (because they contain more information than a single variable) . Another rule is variance cutoffs where we only keep components that explained at least x% of the variance in the data. . Yet another rule is to visualize the eigenvalues in order from highest to lowest connecting them with a line. This is also called the scree plot. . #plot scree plot plt.plot(var_report[&#39;pctVariance&#39;]) . . From visual inspection, we shall keep all the component whose eigenvalues or pctVariance values falls above the point where the slope of the line changes the most drastically.This is also called the ‘elbow’ point.Thus 24% and above will form our components. . Conclusion . In the course of this practical post, we have come to understand the significance of PCA operation in the machine learning process. We also demonstrate the simplest and effective way of analyzing the PCA decision process, which, actually is the most important step, as the primary purpose for undertaking a PCA operation invariably is to reduce features! . I hope you have learned something today. Bye! .",
            "url": "https://ajakaiye33.github.io/DataMusings/markdown/2020/11/09/pca.html",
            "relUrl": "/markdown/2020/11/09/pca.html",
            "date": " • Nov 9, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Framework of Power Theft Detector Model",
            "content": ". Introduction . I consider myself an incurable continuous learner. You won’t agree with me less when I say the capacity and ability to learn, re-learn and unlearn is a crucial skill in this current era of information explosion. Little wonder when Edvancer Venture and University of Petroleum and Energy Studies combined to create the Post Graduate Professional Certificate in Data science; I seized the opportunity and enrolled as soon as possible. . The course is group into the following broad areas: . Fundamental knowledge about Data Science, Data Analytics, Machine Learning and their importance to businesses and organizations . | Introduction to the primary programming language of Data Science and Machine Learning: R &amp; Python . | Hands-on and practical session on data handling cleaning and Model building . | Student Capstone Project wherein students apply all they have learned to solve data-related problems in the following sectors: . Aviation | Banking &amp; Finance | E-Commerce | Logistics &amp; Supply Chain | Energy | . | . My favourite aspect of the program is the Capstone Project session because it provides an opportunity to practice on real-life datasets. I chose the Capstone on Energy as the problem of Energy theft was particularly dear to my heart. I have withness the problem first hand in my former residence, and I could remember asking why was such impunity not detected and the culprit made to face the consequences of the crime of Energy theft as enshrined in the Law. Thus curiosity and the passion to apply my newly acquired skills to practical use informed my decision to work on Energy Theft Detector Model. . Consequently, today’s post is a chronicle of how I built an Energy theft Detector model. For easy comprehension, we divided the project into the following sub-themes: . Making Sense of the Data | Data cleaning and pre-processing | Correlations and Outliers | Check Feature Importance | Simple Base-model | Building model | . Import Necessary Packages and Modules . The knowledge of the requisite package to use in a given project or problem depends on the experience of the practitioner and the level of work to be done. . . Making Sense of The Data . In any machine learning or data science project, the data is about the most important aspect. The level to which you understand the data is base on domain knowledge of the practitioner. I had to go the extra mile to understand the data and all of its variable. Data research is essentially necessary when the problem is not under your primary domain knowledge. To this end, I read far and wide to get a sense the meaning behind the various variables and ultimately produce a data dictionary: . . Data Cleaning and Preprocessing . Load data into the pandas package and display a small portion . elect_data = pd.read(&#39;Power_theft.csv&#39;) elect_data.head() . . From the above data, our target variable is global_active_power. In other words, global_active_power is our dependent variable, whereas other variables are our independent variables. Since our dependent variable data type is float or decimal, our model would invariably be linear regression, a prevalent supervised learning machine learning model. Before any further operations on the data, it is necessary to check for missing values and treat accordingly. . . . Correlation and Outlier . Before I go further into the procedures and steps taken in the course of building the model, I will like to stress an essential concept I learned about the dependent variable. From my research, I decipher that ‘active power’ is a useful power; thus, the energy that powers our electronics, bulb, heater e.t.c. In an ideal system, the amount of power supplied should equate the amount of power utilized within a given period. Consequently, the simplest means to detect irregular consumption of power would be through outlier detection. Thus Utility customers with an unusual amount of consumption of electricity, especially when compared with the amount of electricity supplied, such usage or consumption may be deem or considered suspicious. . . From the above visualization we understand the correlation coefficient of the various variables. However, we stand a better chance of understanding the correlation of other variables, i.e. independent variables with the target variable or independent variable. . . . We can see very clearly that there are a few irregulars activate power amount via the above box plot. . Check Feature Importance . Knowing the key features of your model would go a long way to improve the overall performance of the model. It would also provide rich information about the features that can make or break our model. Tree-based algorithms like Decision tree, Random Forest e.t.c are suitable for knowing essential features in your models. . . Base Model . Having a base model is a critical step to the art and craft of model building. Through a base model, the practitioner can effectively measure and compare model performance and improvement. We can implement a base model with the scikit-learn dummy model function. Personally, I usually, if it is a Linear Regression problem, customize mine as follows: . # Find the mean of the target variable meansimple = elect_theft_trim[&#39;global_active_power&#39;].mean() print(meansimple) #We use the numpy repeat function to simulate a predicition y_base = elect_theft_trim[&#39;global_active_power&#39;].values y_pred = np.repeat(meansimple,len(elect_theft_trim[&#39;global_active_power&#39;])) #let also show that R_square is equal to zero. r2_score(y_base,y_pred) . Build Various Random Model . A popular cliche in machine learning says: All models are wrong, but some are useful. The wisdom behind this axiom stems from the fact that as a practitioner no one model is right for a problem hence one must cultivate the habit of building several models as a veritable means of knowing which may be useful or practical for the situation or data at hand. The approach is to create a different version of the primary dataset base on other functionalities carried out on the dataset. For instance, in the Power theft problem, I made the following versions: . Original Data: This usually the untampered version of our original datasets | Standardized Data: Some school of thought opined that regression model performs better when the data is Standardized | PCA Reduction Data: There may also be the need to reduce the variables of datasets through Principle Component Analysis. | . I usually pass these versions through various algorithms via a giant loop to check their respective performance. I call this process, Out-of-the-box-models. . . From the above, we can choose the models with the best F1 score for further tuning and improvement. The above steps are the way and manner I approach model Building and machine learning task. I hope you have learned something today. Bye! .",
            "url": "https://ajakaiye33.github.io/DataMusings/markdown/2020/11/02/power_theft.html",
            "relUrl": "/markdown/2020/11/02/power_theft.html",
            "date": " • Nov 2, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Handling Outlier in Data Analysis",
            "content": "Introduction . . I have to be frank; the only time I thoroughly understood the concept of outliers was when I read Malcolm Gladwell’s book: ‘Outliers’. Although I have come across the word in my undergraduate statistics class, the concept never sunk into my subconscious. I wasn’t lucky. My statistics lecturer taught statistics purely on a theoretical level with little or no connection with reality or real-life experience and application! . Gladwell borrowed the concept from the field of statistics and so with the help of ‘connection’ I gained a deeper understanding about one of the critical concept in statistics and by extension data science and machine learning! Today’s post shall therefore dwell on outliers, however, from the perspective of data preprocessing in data science and machine learning. To this end, we would cover: . Meaning and definition of an outlier from data science and machine learning perspective | The implication of outlier in our dataset if not treated | Practical ways to detect and handle outliers | . Outlier is a data point that differs significantly from other observations - Wikipedia . The above definition is quite apt and concise. From the angle of normal distribution and empirical rule, it is those data point which falls outside the three standard deviations of the mean. Care, however, should be observed to avoid assumption. Outliers are relatively amorphous; thus, we should tread carefully in an attempt to designate data points as outright outliers without rigorous investigation. . There is a situation where outliers play a critical function. Of course, this is dependent on the objective of the analysis, for example, where the study objective is to detect fraud in electricity usage. Under this scenario here, the concept of outlier would play a critical role. On the other hand, outlier could be an ‘experiment error’ in which case we should treat it accordingly else we risk erroneous interpretation and inference. . Identifying Outliers . Just as knowing or identifying a problem is the first step toward its solution. Similarly, to treat and handle outliers effectively, we must have the means of detecting it. . Detecting outliers through constructing a box plot | Through the IQR proximity rule | Through Z score calculation | . Since we try to be as practical as possible, we shall illustrate and demonstrate the above techniques with an example. . Under the IQR proximity rule, a value is an outlier if it falls outside these boundaries: upper_boundary = 75th quantile + (IQR *1.5) lower_boundary = 25th quantile - (IQR * 1.5) . Where IQR is calculated as IQR = 75TH quantile - 25th quantile However for an extreme case outlier we would multiply IQR by 3 instead of 1.5 . import pandas as pd import matplotlib.pyplot as plt import numpy as np import seaborn as sns from sklearn.dataset import load_boston from scipy.stats import zscore . Load the Boston House Prices dataset from scikit-learn and retain three of its variable in the data frame . bost_dataset = load_boston() boston = pd.DataFrame(bost_dataset.data, columns=bost_dataset.feature_names)[[&#39;RM&#39;,&#39;LSTAT&#39;, &#39;CRIM&#39;]] . Make a boxplot for the RM variable . sns.boxplot(y=boston[&#39;RM&#39;]) plt.title(&#39;Boxplot&#39;) . . create a function that takes in a DataFrame the factor to use for the IQR and a variable name and returns the IQR proximity rule boundaries . def find_bound(df,var,distance): IQR = df[var].quantile(0.75) - df[var].quantile(0.25) lower_bound = df[var].quantile(0.25) - (IQR * distance) upper_bound = df[var].quantile(0.75) * (IQR * distance) return upper_bound, lower_bound calculate and display the IQR proximity rule boundaries for *RM* variable python upper_bound, lower_bound = find_bound(boston, &#39;RM&#39;, 1.5) . This returns the values above and below that are considered as outlier . We can create a boolian vector to flag observation outside the boundaaries we previously established . outliers = np.where(boston[&#39;RM&#39;] &gt; upper_bound, True, np.where(boston[&#39;RM&#39;] &lt; lower_bound, True, False)) . We can equally create a dataframe with the outlier values and display the top five values . outlierdf = boston.loc[outliers, &#39;RM&#39;] outlierdf.head() . Using zscore to detect outliers in RM variable . outlierz = zscore(boston[&#39;RM&#39;]) # Decision: where the value +3 and above then it is an Outliers . Conclusion . The way and manner outliers are handled and treated would depend on the objective of the analysis or problem statement of the study. Overall when not handle appropriately, outlier could lead to erroneous inferences especially in the area of central tendency measurement as some of such measurement could be sensitive to inexplicably large values. Hope you learn something today. Bye! .",
            "url": "https://ajakaiye33.github.io/DataMusings/markdown/2020/10/26/outlier.html",
            "relUrl": "/markdown/2020/10/26/outlier.html",
            "date": " • Oct 26, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Feature Engineering With Feature-Engine Package(1)",
            "content": ". Introduction . I can remember in my last post I render some popular definitions of Data science that are out there. In one such description, I referred to the fact that data science was not just a science but an art. You may be curious: ‘How is Data Science an art?’ From my perspective, art is more or less an unstructured process, or without a universal modus operandi but centred on individual expressiveness and creativity. An aspect of Data science where artistry and creativity comes to play is no other than Feature Engineering . Therefore this post is about the art and craft of feature engineering, a necessary and fundamental activity and process under data cleaning and preparation. It is an essential process, especially when you need to prepare data for machine learning. No doubt Feature Engineering is a broad topic however my objective is to highlight some salient and practical aspect using Feature-Engine a python package that is fast gaining momentum over the popular scikit-learn package. . For sufficient coverage would break this topic into a series wherein different aspect of feature engineering would be highlighted and how the package feature engine makes such process neat clean and simple compared to the conventional scikit-learn package. . Handling and Imputing Missing Data. . You will agree with me that this is one area in the data cleaning process that gives a lot of machine learning newbies sleepless nights! As a way of definition, the act of replacing missing data with a statistical estimate of missing values is called imputation. The primary goal of any imputation technique is to have a dataset used to train machine learning models. The form of imputation employed usually would depend on the following: . Is the data missing at random | The number of missing values | and the type of machine learning model to be used or apply | . Common steps and forms of handling missing data . In the course of the post, we shall demonstrate and discuss the following ways/methods: . Removing observations with missing data | Mean or median imputation | Mode or frequent category imputation | . . Install Import and load necessary package and datasets . To perform and demonstrate the above processes, we would need the python packages: . Numpy | Pandas | Scikit-learn | Feature-Engine | . You can have the above packages in one fell swoop when you install the free Anaconda Python distribution here . Also to know more about Feature-engine visit here . Meanwhile shall use the crx.data for illustration . # import necessary packages import random import pandas as import pd import numpy as np . Modify dataset to suite our purpose . # load the data data = pd.read_csv(&#39;crx.data&#39;, header=None) # create a list with variable names avr_name = [&#39;A&#39; + str(s) for s in range(1,17)] #put the variable names to dataframe data.columns = avr_name #replace the question marks(?) in the dataset with numpy NaN values: data = data.replace(&#39;?&#39;,np.nan) #recast the numerical variable as float data types data[&#39;A2&#39;] = data[&#39;A2&#39;].astype(&#39;float&#39;) data[&#39;A14&#39;] = data[&#39;A14&#39;].astype(&#39;float&#39;) # recode the target variable as binary data[&#39;A16&#39;] = data[&#39;A16&#39;].map({&#39;+&#39;:1, &#39;-&#39;:0}) #add some missing values random.seed(42) values = set([random.ranint(0,len(data)) for p in range(0,100)]) for avr in [&#39;A3&#39;,&#39;AB&#39;,&#39;A9&#39;,&#39;A10&#39;]: data.loc[leaves,avr] = np.nan # Save your prepared data data.to_csv(&#39;dredit_app.csv&#39;,index=False) . Removing observation with missing data . This method is also called Complete Case Analysis The above method involves discarding those observations where the values in any of the variable are missing. We can apply this method in categorical and numerical variables. Although this process is fast and direct but could lead to throwing away a significant amount of data mostly when the value is missing across variables . #calculate the percentage of missing values and display in descending manner data.isna().mean().sort_values(ascending=True) #remove the observation with missing data in any of the variables data_clean = data.dropna() # check to verify through data shape data_clean.shape, data.shape . Mean and Median imputation . This method is about the popular and somewhat logical. Its essentially involve replacing missing values with the variable mean or median. Under this method, we would show how to replace missing values with the mean or median using scikit-learn and Feature-engine. The best approach as a way of preventing data leakage is to calculate the mean or median using the train part of the data set but apply the result to both datasets: train and test. . #import necessary packages from sklearn.model_selection import train_test_split from sklearn.impute import SimpleImputer from feature_engine.missing_data_imputers import MeanMedianImputer . we will continue with our data as above . Xtrain, X_test, y_train, y_test = train_test_split(data.drop(&#39;A16&#39;,axis=1), data[&#39;A16&#39;],test_size=0.3,random_state=42) # check the percentage of missing vales in the training set Xtrain.isna().mean() # replace the missing values with the median in five numerical variables using pure pandas for mix in [&#39;A2&#39;,&#39;A3&#39;,&#39;A8&#39;,&#39;A11&#39;,&#39;A15&#39;]: med_val = Xtrain[mix].median() Xtrain[mix] = Xtrain[mix].fillna(mid_val) X_test[mix] = X_test[mix].fillna(mid_val) . Using scikit-learn’s SimpleImputer to replace missing values with median . imputer = SimpleImputer(strategy=&quot;mean&quot;) #fit the SimpleImputer to the train set so ot learn the median values imputer.fit(Xtrain) # replace missing values with median Xtrain = imputer.transform(Xtrain) X_test = imputer.transform(Xtrain) . Now lets see the simplicity of feature_engine package carrying same processes . median_imputer = MeanMedianImputer(imputation_method=&#39;median&#39;), variables = [&#39;A2&#39;,&#39;A3&#39;,&#39;A8&#39;,&#39;A11&#39;,&#39;A15&#39;] #fit the median imputer to learn the median from the variables median_imputer.fit(Xtrain) # replace the missing values with median Xtrain = median_imputer.transform(Xtrain) X_test = median_imputer.transform(Xtest) . One way the feature_engine is better is that it by default return a dataframe after such imputation . Mode or frequent category imputation . This method involves replacing missing values with the mode. This method is common in categorical variables. . #import necessary packages from feature_engine.missing_data_imputers import FrequentCategoryImputer . As usual, we shall continue with our data as above. Don’t forget to split the dataset into train and test parts. We would go straight and show how this procedure is implemented with feature_engine as similar procedure as used above with pandas, and scikit-learn. . mode_imputer = FrequentCategoryImputer(variables=[&#39;A4&#39;,&#39;A5&#39;,&#39;A6&#39;,&#39;A7&#39;]) # fit the imputation transformer to the train set to learn the most frequent categories mode_imputer.fit(Xtrain) # inspect the the learned frequent categories mode_imputer.imputer_dict_ # replace the missing values with frequent categories Xtrain = mode_imputer.transform(Xtrain) X_test = mode_imputer.transform(X_test) . Conclusion . I hope you are as astonished as I was when I first discovered feature_engine package. The good thing about this python package is what I referred to as ‘specialized attention’ to feature engineering for machine learning. I hope you learned something today. Bye! .",
            "url": "https://ajakaiye33.github.io/DataMusings/markdown/2020/10/19/feature.html",
            "relUrl": "/markdown/2020/10/19/feature.html",
            "date": " • Oct 19, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Beginning Data Analysis And Data Science",
            "content": ". Introduction . Data science as a concept and practice has many definitions. Some definitions out there are: ‘ Data Science is the sexiest field of study and or career’. ‘ Data Science is a Multidisciplinary study anchored on domain knowledge’. ‘ Data Science is an intersection of Statistics, Computer Science, Mathematics, Software Development and Domain expertise’. e.t.c . You would agree that Data Science is an exceptionally diverse and exciting field which marries both worlds of art and sciences. My favourite definition is that which considers Data Science as a Multidisciplinary study because, in my opinion, Data Science is not an end of itself but a means to an end! On that note, I welcome you, yes, you reader- to this week’s post which promises to be exciting and educative. . In the preceding lines, I surmised that Data Science is a means to an end and not the other way round because, from my experience, its application transcends many filed of study and careers. To this end, any domain expert or domain newbie can apply the power of data science to solve problems unique to that domain. As much as I acknowledge these characteristics as plausible, there are some drawbacks and shortcomings. However, the most glaring one is the tendency to attract individuals with little or no foundation in Statistics, Mathematics and coding skills. My attention is on such category of persons because I believe with some structure anybody irrespective of their knowledge of Statistics, Mathematics or python skills can apply and unleash the powers of data science. Consequently, the focus of this post is on the initial steps that you observed as you decide to solve a Data Science problem. I classify these steps into two broad parts: . Pre Data steps | Data steps | . . Pre-Data Steps . While this step is the most crucial in the data science circle, it is equally the most overlooked or neglected. A typical data science circle may involve: . Understanding product snd client | Data Exploration | Hypothesis formation | Data Massaging | Experimenting | Communicating results | and Iterating | . From the above steps, not for nothing does a pre-data step- Underderstanding product and clients- comes first. It’s a critical step that formed the foundation of other steps. Understanding product and clients mean being abreast with the goals and objective of the problem. To cover this step sufficiently, you may ponder and answer the following questions: . why is this problem important? as they say there is a story behind a story This may involve carrying out a need analysis from the perspective of the client or the owner of the data | Is there enough data? | How can performance be measured or the Key Performance Indicator | Will the solution be descriptive and or prescriptive? | Does the problem require data science or machine learning? | what tools or environment available or affordable | e.t.c | . Of course, this list is by no means exhaustive. The more questions, the more profound will your understanding of the clients and products. . . Data Steps . Under this phase, the probing moves from theoretical to more technical. . Meta Data | Data preprocessing | . Meta Data is simply data about data, and the following process can be used under a jupyter/pandas environment to understand the metadata of our data set. . Load the data unto jupyter using the appropriate pandas’ method to read your data accordingly. Assuming the data is in CSV format, we would read and saved it to a variable . import pandas as pd import numpy as np . data_read = pd.read_csv(&#39;path to data on local machine or external url&#39;)` . data_read.shape #get the numbers of columns and rows of data . data_read.info() # displays detailed info about the data set` . data_read.dtypes # displays the various data types of the variables in of the data sexiest . data_read.describe(include=[np.number]).T # Get the summary Statistics for the numerical variable and transpose the DataFrame . data_read.describe(include =[np.object, pd.Categorical]).T # Get the summary Statistics for the object and Categorical columns . data_read.memory_usage(deep=True) # display the space occupied by the various data variables and types . Data preprocessing include any and every data acrobatics deemed necessary to shape the required data for further analysis or machine learning. However, the entire Data steps can be subsumed under data exploration, as noted under the data science life circle. . Conclusion . The field of data science is a growing field of study and career. Because of this dynamism, processes and procedures changes by the speed of light, and the best attitude is to have an open mind and be ready to learn to unlearn and re-learn. And talking about education, I hope you have learned something, however small from this post. Bye! .",
            "url": "https://ajakaiye33.github.io/DataMusings/markdown/2020/10/12/approach_data.html",
            "relUrl": "/markdown/2020/10/12/approach_data.html",
            "date": " • Oct 12, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Wrangling Data With Pythons Generative Expressions",
            "content": ". Introduction . In recent times we have heard it in different quarters that data is the new oil You will agree that this is a lofty designation that would elicit if anything some eye-rolling episodes :smirk: . However I must point out as an aspiring Data Scientist &amp; Wrangler, I believe the assertion. The importance of data especially in the 21st century can not be overemphasize. Nevertheless, if you aren’t convinced just yet, I suggest you read Yuval Noah Harari’s Homo Deus :closed_book: . To continue with the above analogy, it’s evident that oil goes through various processes and transformation before it’s certified useful in the multiple ways that we currently oil. . Similarly, Data is basically useless in its crude form until it is transformed. The process of transforming Data has been dubbed as Data Cleaning or Data Scrubbing or Data Wrangling. The python language has a lot of tools that make this cleaning process easy. I thought I’m versed in the available data wrangling tools until when I discovered Generative Expressions Therefore this post will be about How efficient and elegant is python’s Generative Expressions especially when compared to other methods like for loop, list comprehension etc.in the art and craft of Data Wrangling! . Efficiency &amp; Effectiveness . According to Peter Drucker, the father of management, effectiveness is doing the right things, efficiency is doing things rightly. This definition may sound confusing at first, but on deeper reflection, it actually hit the nail on the head. You can be effective without necessarily be efficient. To demonstrate this concept. I will use a real-life code examples. Firstly, I will need to import an essential function from the sys python package. . # from sys import getsizeof . Extract odd number with for loop . odd_num_with_FL = [] for i in range(100000): if i % 2 != 0: odd_num_with_FL.append(i) . Extract odd number with List Comprehension . # get odd number odd_num_with_LC = [x for x in range(100000) if x % 2 !=0] . Extract odd number with generative expressions . # get odd number using generative expressions odd_num_with_GE = (y for y in range(100000) if y % 2 !=0) . The above three different ways we extracted the odd numbers are correct and effective but are all three examples efficient? . One way to measure code efficiency is the number of bytes an operation occupies in the computer memory. Without further ado, we can know the size our above code occupies in our system’s memory with the aid of getsizeof function from the sys package. . Let us see the bytes that each operation takes: . print(getsizeof(odd_num_with_FL)) . 406496 bytes . print(getsizeof(odd_num_with_LC)) . 406496 #bytes . print(getsizeof(odd_num_with_GE)) . 120 #bytes . Wow! isn’t this amazing! Look at the difference in bytes. I understand. You aren’t alone. I was blown away when I encounter the power of the generative expression. . However, the magic behind this apparent efficiency lies in the concept of lazing rendering or evaluation as the generative expression saves little or nothing in the memory. You don’t believe me … let us see the code. . print(odd_num_with_GE) . &lt;generator object at 0x10453a840&gt; . Unlike the output of the above code, the other code examples would print out the actual odd numbers, you can go ahead and print them out. Knock yourself out! :smile: . Elegance and Simplicity . Efficiency, which we have tried to illustrate in the preceding session, could also be defined in the light of simplicity. Not for nothing did Austin Freeman, said Simplicity is the Soul of efficiency. Similarly, according to Leonardo Da Vinci, Simplicity is the ultimate sophistication and I make bold to say you don’t understand a concept or a phenomenon until you can render it as simple as possible. The same simplicity informs the Zen of python. . We don’t need to complicate the already complicated process of Data Wrangling and Data Cleaning, to this end you can not but agree that any procedure or tool seeking to reduce the observed complexity would be welcome and embraced by all and sundry. The generative expression makes life easy, even in the thick and thin of the dirtiest of data. Let illustrate this with the following code: . # Clean the following List of words words = [&#39;Hello n&#39;, &#39;My name&#39;, &#39;is n&#39;,&#39;Bob&#39;,&#39;How are&#39;, &#39;doing n&#39;] . Using the normal for loop to clean the above list of words . clean_words = [] for word in words: for w in word.split(&#39; &#39;): clean_words.append(w.strip().lower()) . print(clean_words) [&#39;hello&#39;, &#39;my&#39;, &#39;name&#39;,&#39;is&#39;,&#39;bob&#39;,&#39;how&#39;,&#39;are&#39;,&#39;you&#39;, &#39;doing&#39;] . Without the risk of sounding immodest, the above path would be the route taken by the majority of python newbies in solving this problem. This approach may be very much useful but appears ponderous. Just look at the number of lines of code written and the complex nested for loop. Can the above code be improved on with generative expression? . Here is the code: . clean_words_with_GE = (word.strip().lower() for word in words for w in word.split(&#39; &#39;)) . The same result was achieved with just a line and also little, or no memory is utilized by lazy evaluation. . Brevity Vs Readability . There is no doubt that the generative expression approach not only engenders efficiency, it also ensures simplicity. Like every other approach, it has its own drawbacks and limitations. And chief amongst the limitations is Readability. Our code may not pass the 7th Zen of Python’s rule, which admonished:’Readability Counts’. Therefore as you enjoy the new powers of generative expression equally ensures that your code is clear and readable. . I hope you enjoy this post. Till we meet again, happy coding:hammer: .",
            "url": "https://ajakaiye33.github.io/DataMusings/markdown/2020/10/05/Gen_expression.html",
            "relUrl": "/markdown/2020/10/05/Gen_expression.html",
            "date": " • Oct 5, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "How Pandas's read_excel Saved My Life",
            "content": ". Introduction . As common amongst the majority of individuals under the category of Data Scientist by Accident, once in a while, you come across what I call ‘moments of enlightenment’ which more or less upgrade your knowledge about the ‘tools of the trade’. I encountered one of such moments a couple of days ago. And since I just set up a new blog courtesy of fastai, I decided to relay my experience on how I entered Zen from discovering the usefulness and existence of pandas’s read_html methods! . Data in the Data Science Process . I make bold to say that the entire Data Science process revolves around one main resource: DATA. Data is an essential raw material in the manufacturing of Data Visualization Models. While drawing on the analogy to drive home my point about data and its importance in the data Science pipeline, I’m very much aware that we don’t literally manufacture anything rather a methodical transformation of inputs takes place. Thus raw data as an input is transformed into an output format that solves one problem or needs in the society. No doubt data is very important in the data Science process, but how do we go about getting this data into our data science work environment? . Getting Data into Jupyter Notebook . Before encountering Data Science, all I know about data and its handling was tied to one tool: MS EXCEL. Excel is an excellent tool in it own right but there come a time in your data handling experience when excel won’t just cut it. Most of my challenges with excel started manifesting when my data became larger. Lucky for me after narrating my challenges on stackoverflow and exposing myself to public ridicule from some not-too-kind responders, I went away with the knowledge of the existence of PANDAS and JUPYTER NOTEBOOK. Of course, with these new set of tools, I became the go-to-guy in the office for any data wrangling exercise and functions. Majority of the data I use comes from scraping them off websites pages and transforming the data into csv format which I then read into the jupyter notbook for onward processing, exploration and analysis. Aside from the pandas’s read_csv method I knew from reading the documentation I could use read_json, read_table, read_html e.t.c to load data unto jupyter notbook but never attempted using them until recently when I attended an online workshop, and the facilitator used the pandas’s read_html method. . ### Can Life be this fun and Easy? My excitement knew no bounds when it occurred to me that I can improve productivity levels by just reading and loading data directly from websites pages especially when the data I’m interested in are housed within html’s table tags. For example, to scrape this site normally, I will write the following code: . def get_site_html(url): try: html = requests.get(url) except HTTPError as e: return None try: bs = BeautifulSoup(html.content, &#39;html.parser&#39;) title = bs except AttributeError as e: return None return title def get_data(): site_html = get_site_html(&#39;https://covid19.ncdc.gov.ng/&#39;) the_table = site_html.find(&#39;table&#39;, {&#39;id&#39;: &#39;custom1&#39;}) head_table = the_table.thead.get_text().split(&#39; n&#39;)[2:6] # print(the_table) body_table = the_table.find_all(&#39;tr&#39;) table_head = body_table[0] # print(table_head) table_row = body_table[1:] # print(table_row) headings = [ht.get_text() for ht in table_head.find_all(&#39;th&#39;)] # print(headings) gey = [] all_rows = [table_row[i].find_all(&#39;td&#39;) for i in range(len(table_row))] for j in all_rows: one_row = [] for h in j: one_row.append(h.get_text().replace(&#39; n&#39;, &#39;&#39;)) gey.append(one_row) df = pd.DataFrame(data=gey, columns=headings) return df . As much as I suspect the above sample code code be more concise, with pandas read_html method I can achieve the same purpose with just these few lines: . import pandas as pd data = pd.read_html(&#39;https://covid19.ncdc.gov.ng/&#39;) data.head() . I bet you can’t get more concise than that. These few lines have saved me alot of time and boosted my productivity to the roof. Hope you learnt a thing or two from this post. Watch out for my next post. Bye! .",
            "url": "https://ajakaiye33.github.io/DataMusings/markdown/2020/09/28/readingData.html",
            "relUrl": "/markdown/2020/09/28/readingData.html",
            "date": " • Sep 28, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://ajakaiye33.github.io/DataMusings/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://ajakaiye33.github.io/DataMusings/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://ajakaiye33.github.io/DataMusings/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ajakaiye33.github.io/DataMusings/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}